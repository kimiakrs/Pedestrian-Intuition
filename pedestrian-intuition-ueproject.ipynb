{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14299714,"sourceType":"datasetVersion","datasetId":9128197},{"sourceId":14437945,"sourceType":"datasetVersion","datasetId":9222224},{"sourceId":14453310,"sourceType":"datasetVersion","datasetId":9231652},{"sourceId":14458973,"sourceType":"datasetVersion","datasetId":9235265},{"sourceId":14459140,"sourceType":"datasetVersion","datasetId":9235344},{"sourceId":14486494,"sourceType":"datasetVersion","datasetId":9252701},{"sourceId":14486902,"sourceType":"datasetVersion","datasetId":9252964},{"sourceId":14488119,"sourceType":"datasetVersion","datasetId":9253772},{"sourceId":14488318,"sourceType":"datasetVersion","datasetId":9253906},{"sourceId":14489943,"sourceType":"datasetVersion","datasetId":9254886},{"sourceId":14490383,"sourceType":"datasetVersion","datasetId":9255108}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1- Dataset Discovery (Kaggle Input Check)","metadata":{}},{"cell_type":"code","source":"import os\n\n# This lists everything Kaggle found in your upload\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2- List Available Kaggle Datasets","metadata":{}},{"cell_type":"code","source":"ls /kaggle/input","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3- Install and Import YOLO (Ultralytics)","metadata":{}},{"cell_type":"code","source":"# 1. Install the YOLO library\n!pip install ultralytics\n\n# 2. Re-run your import and training code\nimport os\nimport yaml\nfrom ultralytics import YOLO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4- YOLO Setup, GPU Verification, and Smoke Test Training","metadata":{}},{"cell_type":"code","source":"# 1. Install YOLO (Takes ~30 seconds)\n!pip install ultralytics -q\n\nimport torch\nfrom ultralytics import YOLO\nimport os\nimport yaml\n\n# 2. Verify GPU status\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    \n    # 3. Double-check the YAML exists\n    if not os.path.exists('/kaggle/working/data.yaml'):\n        # Re-create it just in case the session reset\n        data_config = {\n            'train': '/kaggle/input/vehic-ped-intuition/images/train',\n            'val': '/kaggle/input/vehic-ped-intuition/images/val',\n            'test': '/kaggle/input/vehic-ped-intuition/images/test',\n            'nc': 1, 'names': ['pedestrian']\n        }\n        with open('/kaggle/working/data.yaml', 'w') as f:\n            yaml.dump(data_config, f)\n\n    # 4. Run the 2-Epoch Test\n    model = YOLO('/kaggle/working/yolov8n.pt')\n    model.train(\n        data='/kaggle/working/data.yaml',\n        epochs=2,\n        imgsz=640,\n        batch=16,\n        device=0\n    )\n    print(\"\\n‚úÖ SMOKE TEST PASSED! You are ready for the final Commit.\")\nelse:\n    print(\"‚ùå GPU NOT DETECTED. Check the right sidebar 'Accelerator' setting.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5- Final YOLO Training Configuration and Execution (Kaggle)","metadata":{}},{"cell_type":"code","source":"\n!pip install ultralytics\n\nimport os\nimport yaml\nimport torch\nfrom ultralytics import YOLO\n\nif torch.cuda.is_available():\n    print(f\"GPU is active: {torch.cuda.get_device_name(0)}\")\n    device_id = 0\nelse:\n    print(\"GPU not found! Training will be slow on CPU.\")\n    device_id = 'cpu'\n\nDATASET_ROOT = '/kaggle/input/vehic-ped-intuition'\nWORKING_DIR = '/kaggle/working'\n\ndata_config = {\n    'train': f'{DATASET_ROOT}/images/train',\n    'val': f'{DATASET_ROOT}/images/val',\n    'test': f'{DATASET_ROOT}/images/test',\n    'nc': 1,\n    'names': ['pedestrian']\n}\n\nwith open(f'{WORKING_DIR}/data.yaml', 'w') as f:\n    yaml.dump(data_config, f)\n\n\nmodel = YOLO('yolov8n.pt')\n\nprint(\"üöÄ Starting Final Training...\")\nmodel.train(\n    data=f'{WORKING_DIR}/data.yaml',\n    epochs=50,\n    imgsz=640,\n    batch=32,            \n    patience=10,\n    save_period=5,\n    name='jaad_final_model',\n    project=f'{WORKING_DIR}/training_results',\n    device=device_id,\n    exist_ok=True\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6- List Current Working Directory Contents ","metadata":{}},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"8- List Current Working Directory Contents","metadata":{}},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"9- Analyze Test Set Video Distribution","metadata":{}},{"cell_type":"code","source":"import os\nfrom collections import Counter\nimport re\n\n# Path to the test images directory\ntest_dir = '/kaggle/input/vehic-ped-intuition/images/test'\nall_frames = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n\n# Extract video IDs from filenames (e.g., video_0051_frame_001.jpg -> video_0051)\nvideo_ids = []\nfor f in all_frames:\n    if 'video_' in f:\n        # Splits the filename to get 'video' and 'XXXX'\n        parts = f.split('_')\n        video_name = f\"{parts[0]}_{parts[1]}\"\n        video_ids.append(video_name)\n\n# Count frames per video\nvideo_counts = Counter(video_ids)\n\nprint(f\"Total Unique Videos in Test Set: {len(video_counts)}\")\nprint(\"-\" * 40)\nfor vid, count in sorted(video_counts.items()):\n    print(f\"üé¨ {vid}: {count} frames\")\n\nprint(\"-\" * 40)\nprint(f\"Grand Total of Test Frames: {len(all_frames)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"10- Final Model Evaluation on Test Dataset","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\n\n# 1. Load your best trained weights\nmodel_path = '/kaggle/working/training_results/jaad_final_model/weights/best.pt'\nmodel = YOLO(model_path)\n\n# 2. Run validation specifically on the TEST split\nprint(\"Starting Final Evaluation on 2,303 test frames...\")\nmetrics = model.val(\n    data='/kaggle/working/data.yaml',\n    split='test',             # Forces the model to use the 'test' folder\n    imgsz=640,\n    batch=32,\n    name='final_test_evaluation',\n    project='/kaggle/working/evaluation',\n    device=0                  # Use GPU\n)\n\nprint(\"\\n\" + \"=\"*35)\nprint(f\"Mean Average Precision (mAP50): {metrics.box.map50:.4f}\")\nprint(f\"Recall (R): {metrics.box.mr:.4f}\")\nprint(f\"Precision (P): {metrics.box.mp:.4f}\")\nprint(\"=\"*35)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"11- Generate and Save Visual Prediction Results ","metadata":{}},{"cell_type":"code","source":"# 3. Save predicted images for visual inspection\nprint(\"Saving sample predictions for the report...\")\nmodel.predict(\n    source='/kaggle/input/vehic-ped-intuition/images/test',\n    conf=0.33,               # Optimal confidence from your F1 curve\n    save=True,\n    max_det=10,\n    name='prediction_visuals',\n    project='/kaggle/working/evaluation',\n    exist_ok=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = \"/kaggle/working/training_results/jaad_final_model/results.csv\"\ndf = pd.read_csv(csv_path)\nprint(df.tail(5))\nprint(\"\\nBest epoch by mAP50:\", df['metrics/mAP50(B)'].idxmax(), \"=>\", df['metrics/mAP50(B)'].max())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/training_results/jaad_final_model/results.csv\")\ndf[['epoch', 'metrics/mAP50(B)', 'metrics/precision(B)', 'metrics/recall(B)']].tail(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"14-  Video-Level Data Leakage Check (Train/Val/Test Overlap)","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\n\ndef video_ids_from(image_folder: str) -> set[str]:\n    \"\"\"\n    Extracts video IDs from filenames like: video_0051_frame_001.jpg -> video_0051\n    Adjust parsing if your naming differs.\n    \"\"\"\n    vids = set()\n    for f in glob.glob(os.path.join(image_folder, \"*.jpg\")):\n        base = os.path.basename(f)\n        if \"video_\" in base:\n            parts = base.split(\"_\")\n            if len(parts) >= 2:\n                vids.add(f\"{parts[0]}_{parts[1]}\")  # e.g., video_0051\n    return vids\n\ntrain_dir = \"/kaggle/input/vehic-ped-intuition/images/train\"\nval_dir   = \"/kaggle/input/vehic-ped-intuition/images/val\"\ntest_dir  = \"/kaggle/input/vehic-ped-intuition/images/test\"\n\ntrain_vids = video_ids_from(train_dir)\nval_vids   = video_ids_from(val_dir)\ntest_vids  = video_ids_from(test_dir)\n\nprint(\"Unique videos:\")\nprint(\"  Train:\", len(train_vids))\nprint(\"  Val  :\", len(val_vids))\nprint(\"  Test :\", len(test_vids))\n\nprint(\"\\nOverlaps (should ideally be 0 for strict video-level split):\")\nprint(\"  Train ‚à© Test:\", len(train_vids & test_vids))\nprint(\"  Val   ‚à© Test:\", len(val_vids & test_vids))\nprint(\"  Train ‚à© Val :\", len(train_vids & val_vids))\n\n# If you want to see which video IDs overlap:\n# print(\"Train‚à©Test IDs:\", sorted(train_vids & test_vids))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**15- Random Test Sample Inference and Visualization**","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os, glob, cv2, shutil, random\n\n# 1) Load the trained YOLO model\nmodel = YOLO(\"/kaggle/working/training_results/jaad_final_model/weights/best.pt\")\n\n# 2) Load all test images\ntest_dir = \"/kaggle/input/vehic-ped-intuition/images/test\"\nall_imgs = glob.glob(os.path.join(test_dir, \"*.jpg\"))\n\n# 3) Output directory (cleaned on every run)\nout_dir = \"/kaggle/working/evaluation/random_samples\"\nif os.path.exists(out_dir):\n    shutil.rmtree(out_dir)\nos.makedirs(out_dir)\n\n# 4) Randomly select test samples\nN = 8            # number of images per run\nCONF = 0.33      # confidence threshold\n\nselected_imgs = random.sample(all_imgs, N)\n\nfor i, img_path in enumerate(selected_imgs, start=1):\n    results = model.predict(\n        source=img_path,\n        conf=CONF,\n        iou=0.5,\n        max_det=10,\n        save=False,\n        verbose=False\n    )\n\n    # Get annotated prediction image (BGR format)\n    annotated = results[0].plot()\n\n    # Save prediction result\n    save_path = os.path.join(out_dir, f\"sample_{i}.jpg\")\n    cv2.imwrite(save_path, annotated)\n\nprint(f\"‚úÖ {N} random prediction samples saved to ‚Üí {out_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lah /kaggle/input/vehic-ped-intuition\n!ls -lah /kaggle/input/vehic-ped-intuition/crops","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****PHASE 2****","metadata":{}},{"cell_type":"markdown","source":"Checking Random Samples","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport torch\n\nDEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n\nyolo = YOLO(\"/kaggle/input/first-phase-model/weights/best.pt\")\nprint(\"YOLO model loaded ‚úî\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"/kaggle/input/first-phase-model/weights/best.pt\")\nimgs = glob.glob(\"/kaggle/input/vehic-ped-intuition/images/test/*.jpg\")\n\ndetections = []\nfor img in imgs[:200]:  # sample\n    r = model.predict(img, conf=0.33, verbose=False)[0]\n    detections.append(len(r.boxes))\n\nprint(\"Avg detections per frame:\", sum(detections)/len(detections))\nprint(\"Zero-detection frames:\", sum(d == 0 for d in detections))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport random\nfrom collections import defaultdict\nfrom ultralytics import YOLO\nimport cv2\n\nTEST_IMG_DIR = \"/kaggle/input/vehic-ped-intuition/images/test\"\nOUT_DIR = \"/kaggle/working/evaluation/random_video_samples\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndef get_video_id(fname):\n    m = re.search(r\"(video_\\d+)\", fname)\n    return m.group(1) if m else None\n\n# Group frames by video\nvideo_frames = defaultdict(list)\nfor f in os.listdir(TEST_IMG_DIR):\n    if f.endswith(\".jpg\"):\n        vid = get_video_id(f)\n        if vid:\n            video_frames[vid].append(f)\n\nprint(\"Total test videos:\", len(video_frames))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random.seed(42)\n\nselected_videos = random.sample(list(video_frames.keys()), 10)\nprint(\"Selected videos:\", selected_videos)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo = YOLO(\"/kaggle/input/first-phase-model/weights/best.pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONF = 0.33\n\nfor vid in selected_videos:\n    frames = video_frames[vid]\n    sampled_frames = random.sample(frames, min(3, len(frames)))\n\n    vid_out = os.path.join(OUT_DIR, vid)\n    os.makedirs(vid_out, exist_ok=True)\n\n    for fname in sampled_frames:\n        img_path = os.path.join(TEST_IMG_DIR, fname)\n\n        results = yolo.predict(\n            source=img_path,\n            conf=CONF,\n            iou=0.5,\n            max_det=10,\n            verbose=False\n        )\n\n        annotated = results[0].plot()\n        save_path = os.path.join(vid_out, fname)\n        cv2.imwrite(save_path, annotated)\n\nprint(\"‚úÖ Saved random detections per video to:\", OUT_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\nSRC_DIR = \"/kaggle/working/evaluation/random_video_samples\"\nZIP_PATH = \"/kaggle/working/random_video_samples.zip\"\n\n# Remove old zip if exists\nif os.path.exists(ZIP_PATH):\n    os.remove(ZIP_PATH)\n\n# Create zip\nshutil.make_archive(\n    base_name=ZIP_PATH.replace(\".zip\", \"\"),\n    format=\"zip\",\n    root_dir=SRC_DIR\n)\n\nprint(\"‚úÖ Zipped to:\", ZIP_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nfrom collections import defaultdict\n\n# ======================================================\n# 1) Paths\n# ======================================================\nTEST_IMG_DIR = \"/kaggle/input/vehic-ped-intuition/images/test\"\n\n# ======================================================\n# 2) Helper: extract video ID\n# ======================================================\ndef get_video_id(filename):\n    \"\"\"\n    Extracts video ID from filenames like:\n    video_0024_frame_0050.jpg -> video_0024\n    \"\"\"\n    m = re.search(r\"(video_\\d+)\", filename)\n    return m.group(1) if m else None\n\n# ======================================================\n# 3) Group frames by video\n# ======================================================\nvideo_frames = defaultdict(list)\n\nfor fname in sorted(os.listdir(TEST_IMG_DIR)):\n    if not fname.lower().endswith(\".jpg\"):\n        continue\n\n    vid = get_video_id(fname)\n    if vid is not None:\n        video_frames[vid].append(fname)\n\n# ======================================================\n# 4) Summary\n# ======================================================\nprint(\"‚úÖ Total test videos found:\", len(video_frames))\n\nfor i, (vid, frames) in enumerate(video_frames.items()):\n    print(f\"{vid}: {len(frames)} frames\")\n    if i >= 9:  # show first 10 only\n        break\n\n# ======================================================\n# 5) Optional: inspect a specific video\n# ======================================================\nTARGET_VIDEO = \"video_0024\"\n\nif TARGET_VIDEO in video_frames:\n    print(f\"\\nüìå {TARGET_VIDEO} contains {len(video_frames[TARGET_VIDEO])} frames\")\nelse:\n    print(f\"\\n‚ùå {TARGET_VIDEO} not found in test set\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nOUT_DIR = \"/kaggle/working/video_0024_test_frames\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nfor fname in video_frames[\"video_0024\"]:\n    src = os.path.join(TEST_IMG_DIR, fname)\n    dst = os.path.join(OUT_DIR, fname)\n    shutil.copy(src, dst)\n\nprint(f\"‚úÖ Copied frames to: {OUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONF  = 0.33      # optimal from F1-confidence curve\nIMGSZ = 960       # improves small/night targets\nAUG   = True      # test-time augmentation\nIOU   = 0.40      # stricter NMS to reduce duplicates\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Enhanced Light ","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport random\nimport numpy as np\nfrom ultralytics import YOLO\n\n# 1. DEFINICI√ìN DE LA FUNCI√ìN DE MEJORA (Soluci√≥n al NameError)\ndef enhance_low_light(img):\n    \"\"\"\n    Mejora la visibilidad en escenas oscuras usando CLAHE en el espacio de color LAB.\n    \"\"\"\n    if img is None:\n        return None\n    # Convertir a LAB para manipular la luminancia (L) sin afectar los colores (A, B)\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    \n    # CLAHE: Ecualizaci√≥n de histograma adaptativa limitada por contraste\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    cl = clahe.apply(l)\n    \n    # Unir canales y volver a BGR\n    limg = cv2.merge((cl,a,b))\n    return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n\n# 2. CONFIGURACI√ìN\nCONF = 0.25  # Un poco m√°s bajo para captar peatones en sombras\nIOU = 0.45\nOUT_DIR = \"/kaggle/working/enhanced_tracking_samples\"\n\n# Asumiendo que ya tienes definidos: selected_videos, video_frames, TEST_IMG_DIR y yolo\nfor vid in selected_videos:\n    frames = video_frames[vid]\n    # Tomamos 5 muestras para ver mejor la continuidad\n    sampled_frames = sorted(random.sample(frames, min(5, len(frames))))\n\n    vid_out = os.path.join(OUT_DIR, vid)\n    os.makedirs(vid_out, exist_ok=True)\n\n    print(f\"üñºÔ∏è Procesando muestras mejoradas para: {vid}\")\n\n    for fname in sampled_frames:\n        img_path = os.path.join(TEST_IMG_DIR, fname)\n        img_orig = cv2.imread(img_path)\n        \n        if img_orig is None: continue\n\n        # APLICAR MEJORA\n        img_enhanced = enhance_low_light(img_orig)\n\n        # USAR TRACK EN LUGAR DE PREDICT (Vital para Fase 2)\n        # persist=True mantiene el ID del peat√≥n entre frames\n        # classes=[0] filtra para que SOLO detecte personas\n        results = yolo.track(\n            source=img_enhanced,\n            conf=CONF,\n            iou=IOU,\n            persist=True,\n            classes=[0], \n            verbose=False\n        )\n\n        # Dibujar anotaciones (incluye el ID del tracking)\n        annotated = results[0].plot(line_width=2)\n        \n        # Agregar texto informativo en la imagen\n        cv2.putText(annotated, f\"ID Tracking Habilitado | Peatones Solo\", (20, 40), \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n\n        save_path = os.path.join(vid_out, f\"enhanced_{fname}\")\n        cv2.imwrite(save_path, annotated)\n\nprint(f\"‚úÖ Proceso completado. Revisa las im√°genes en: {OUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The CLAHE is being effective, getting more continual vision, the AVG streak almost duplicated itself","metadata":{}},{"cell_type":"code","source":"def streak_lengths(counts):\n    streaks = []\n    cur = 0\n    for c in counts:\n        if c >= 1:\n            cur += 1\n        else:\n            if cur > 0:\n                streaks.append(cur)\n            cur = 0\n    if cur > 0:\n        streaks.append(cur)\n    return streaks\n\norig_streaks = streak_lengths(orig_counts)\nenh_streaks  = streak_lengths(enh_counts)\n\nprint(\"Original streaks:\", orig_streaks)\nprint(\"Enhanced streaks:\", enh_streaks)\nprint(\"Original avg streak:\", np.mean(orig_streaks) if orig_streaks else 0)\nprint(\"Enhanced avg streak:\", np.mean(enh_streaks) if enh_streaks else 0)\nprint(\"Streaks ‚â•3 (orig):\", sum(s >= 3 for s in orig_streaks))\nprint(\"Streaks ‚â•3 (enh): \", sum(s >= 3 for s in enh_streaks))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Preparar los datos\nframes = np.arange(len(orig_counts))\n\n# 2. Crear la gr√°fica\n# Quitamos plt.figure() si est√°s en algunas versiones de entorno limitado, \n# pero en Kaggle/Colab funciona bien.\nplt.figure(figsize=(12, 4))\nplt.plot(frames, orig_counts, label=\"Original (Oscuro)\", marker=\"o\", color='blue', alpha=0.7)\nplt.plot(frames, enh_counts, label=\"Enhanced (CLAHE)\", marker=\"s\", color='green', alpha=0.7)\n\n# L√≠nea de referencia (m√≠nimo 1 detecci√≥n)\nplt.axhline(1, color='red', linestyle=\"--\", alpha=0.5, label=\"Umbral de detecci√≥n\")\n\n# Etiquetas y T√≠tulo\nplt.xlabel(\"√çndice del Frame\")\nplt.ylabel(\"N√∫mero de Peatones Detectados\")\nplt.title(\"Efecto de la Mejora CLAHE en Video Nocturno\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Mostrar o Guardar\nplt.tight_layout()\nplt.show() \n# plt.savefig(\"comparativa_deteccion.png\") # Opcional: para descargar la imagen","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Quality Filter","metadata":{}},{"cell_type":"code","source":"def get_streak_indices(counts, min_len=3):\n    streaks, cur = [], []\n    for i, c in enumerate(counts):\n        if c >= 1:\n            cur.append(i)\n        else:\n            if len(cur) >= min_len:\n                streaks.append(cur)\n            cur = []\n    if len(cur) >= min_len:\n        streaks.append(cur)\n    return streaks\n\nlong_streaks = get_streak_indices(enh_counts, min_len=3)\nlong_frames  = sorted(set(i for s in long_streaks for i in s))\nsingle_frames = [i for i, c in enumerate(enh_counts) if c == 1][:5]\nfail_frames   = [i for i, c in enumerate(enh_counts) if c == 0][:5]\n\nselected = sorted(set(long_frames + single_frames + fail_frames))\nprint(\"Selected frames:\", selected)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EVIDENCE_DIR = \"/kaggle/working/night_evidence\"\nos.makedirs(EVIDENCE_DIR, exist_ok=True)\n\nfor i in selected:\n    fname = img_files[i].split(\"/\")[-1]\n\n    shutil.copy(\n        os.path.join(OUT_ORIG, fname),\n        os.path.join(EVIDENCE_DIR, f\"{i:03d}_orig.jpg\")\n    )\n    shutil.copy(\n        os.path.join(OUT_ENH, fname),\n        os.path.join(EVIDENCE_DIR, f\"{i:03d}_enh.jpg\")\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ZIP_PATH = \"/kaggle/working/night_detection_evidence.zip\"\n\nif os.path.exists(ZIP_PATH):\n    os.remove(ZIP_PATH)\n\nshutil.make_archive(\n    ZIP_PATH.replace(\".zip\", \"\"),\n    \"zip\",\n    EVIDENCE_DIR\n)\n\nZIP_PATH\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 2 : Preparing Data for Combining YOLO + ViT + LSTM","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:46:45.683758Z","iopub.execute_input":"2026-01-13T20:46:45.684506Z","iopub.status.idle":"2026-01-13T20:46:50.961644Z","shell.execute_reply.started":"2026-01-13T20:46:45.684477Z","shell.execute_reply":"2026-01-13T20:46:50.960923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing to find out Tracking + Cropping","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport torch\nimport numpy as np\nfrom ultralytics import YOLO\nfrom torchvision import transforms\nfrom PIL import Image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:48:45.999552Z","iopub.execute_input":"2026-01-13T20:48:46.000319Z","iopub.status.idle":"2026-01-13T20:48:49.623239Z","shell.execute_reply.started":"2026-01-13T20:48:46.000265Z","shell.execute_reply":"2026-01-13T20:48:49.62249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Smoke test for enhanced video, tracking and feature extraction to CSV","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport xml.etree.ElementTree as ET\nfrom ultralytics import YOLO\nfrom collections import defaultdict\n\n# ==========================================\n# 1. CONFIGURACI√ìN Y RUTAS\n# ==========================================\nVIDEO_ID = \"0161\"\nPATHS = {\n    'model': '/kaggle/input/first-phase-model/weights/best.pt',\n    'images': '/kaggle/input/vehic-ped-intuition/images/test',\n    'labels': '/kaggle/input/vehic-ped-intuition/labels/test',\n    'attributes': '/kaggle/input/attributes-label/annotations_attributes'\n}\n\nyolo = YOLO(PATHS['model'])\n\n# ==========================================\n# 2. FUNCIONES DE APOYO (Mejora y Handshake)\n# ==========================================\ndef enhance_low_light(img):\n    if img is None: return None\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    cl = clahe.apply(l)\n    return cv2.cvtColor(cv2.merge((cl,a,b)), cv2.COLOR_LAB2BGR)\n\ndef calculate_iou(boxA, boxB):\n    xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])\n    xB, yB = min(boxA[2], boxB[2]), min(boxA[3], boxB[3])\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n\n# ==========================================\n# 3. PROCESAMIENTO DE TRACKING\n# ==========================================\n# Cargar Metadatos XML\nxml_path = os.path.join(PATHS['attributes'], f\"video_{VIDEO_ID}_attributes.xml\")\npeds_meta = {}\ntree = ET.parse(xml_path)\nfor p in tree.getroot().findall('pedestrian'):\n    peds_meta[p.get('id')] = {'dp': int(p.get('decision_point')), 'crossing': int(p.get('crossing'))}\n\n# Obtener Frames\nframe_files = sorted([os.path.join(PATHS['images'], f) for f in os.listdir(PATHS['images']) \n                     if VIDEO_ID in f and f.endswith('.jpg')])\n\nprocessed_data = defaultdict(list)\nid_map = {}\nassigned_xml_ids = set()\n\nprint(f\"üöÄ Iniciando Tracking Mejorado para Video {VIDEO_ID}...\")\n\nfor fidx, path in enumerate(frame_files[:100]): # Procesamos 100 frames para asegurar el DP\n    img = cv2.imread(path)\n    img_enh = enhance_low_light(img)\n    \n    results = yolo.track(img_enh, persist=True, conf=0.25, classes=[0], verbose=False)[0]\n    \n    txt_path = os.path.join(PATHS['labels'], os.path.basename(path).replace('.jpg', '.txt'))\n    \n    if os.path.exists(txt_path) and results.boxes.id is not None:\n        h, w = results.orig_shape\n        gt_boxes = []\n        with open(txt_path, 'r') as f:\n            for line in f:\n                c = list(map(float, line.split()))[1:]\n                gt_boxes.append([(c[0]-c[2]/2)*w, (c[1]-c[3]/2)*h, (c[0]+c[2]/2)*w, (c[1]+c[3]/2)*h])\n\n        t_boxes = results.boxes.xyxy.cpu().numpy()\n        t_ids = results.boxes.id.int().cpu().numpy()\n\n        for tb, tid in zip(t_boxes, t_ids):\n            if tid not in id_map:\n                for gb in gt_boxes:\n                    if calculate_iou(tb, gb) > 0.3:\n                        for xid in peds_meta.keys():\n                            if xid not in assigned_xml_ids:\n                                id_map[tid] = xid\n                                assigned_xml_ids.add(xid)\n                                break\n            \n            if tid in id_map:\n                xid = id_map[tid]\n                processed_data[xid].append({\n                    'frame': fidx, 'bbox': tb, 'before_dp': fidx <= peds_meta[xid]['dp']\n                })\n\n# ==========================================\n# 4. GENERACI√ìN DE DATASET CSV (Fase 2)\n# ==========================================\nrows = []\nfor xid, frames_list in processed_data.items():\n    if len(frames_list) < 3: continue\n    \n    for i in range(len(frames_list)):\n        curr = frames_list[i]\n        b = curr['bbox']\n        cx, cy, w, h = (b[0]+b[2])/2, (b[1]+b[3])/2, b[2]-b[0], b[3]-b[1]\n        \n        # Features cinem√°ticos\n        vel_x, vel_y, delta_area = (0, 0, 1)\n        if i > 0:\n            p_b = frames_list[i-1]['bbox']\n            vel_x = cx - (p_b[0]+p_b[2])/2\n            vel_y = cy - (p_b[1]+p_b[3])/2\n            delta_area = (w*h) / ((p_b[2]-p_b[0])*(p_b[3]-p_b[1]) + 1e-6)\n            \n        rows.append({\n            'ped_id': xid, 'frame': curr['frame'], 'x': cx, 'y': cy, \n            'vel_x': vel_x, 'vel_y': vel_y, 'delta_area': delta_area,\n            'aspect_ratio': w/h, 'before_dp': int(curr['before_dp']),\n            'label': peds_meta[xid]['crossing']\n        })\n\ndf = pd.DataFrame(rows)\ndf.to_csv(\"dataset_fase2_intencion.csv\", index=False)\nprint(f\"\\n‚úÖ ¬°√âxito! CSV guardado con {len(df)} filas.\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üé¨ Multivideo Processor: Phase 1\n---\n> **Purpose:** Extracting cinematic features from multiple video splits (Train, Val, Test) for LSTM training.\nExtracting features for Train,val and testing after the Enhancing of the images using the CLAHE for identifying better the frame streaks where pedestrians were shown or catched without interruptions. Enhanced the image ilumination performing better for dark videos making them visible for YOLO leading to fewer broken trajectories.\n\nOur LSTM will only receive High Quality Sequences of 5 frames per streak so then will learn with accurate data and no trash.\n\nthe before_dp: will allow to identify the pedestrian intention before the person steps into the road, because is learning from the frames before the real DP\n\nVel_x and Vel_y are directional movement\ndelta_area : If it's >1.0 the pedestrian is getting closer to the camera.\naspect_ratio: Helps detect changes in posture for example from standing to walking.\n","metadata":{}},{"cell_type":"markdown","source":"Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport xml.etree.ElementTree as ET\nfrom ultralytics import YOLO\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:49:16.423708Z","iopub.execute_input":"2026-01-13T20:49:16.424133Z","iopub.status.idle":"2026-01-13T20:49:16.82556Z","shell.execute_reply.started":"2026-01-13T20:49:16.424102Z","shell.execute_reply":"2026-01-13T20:49:16.824776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport xml.etree.ElementTree as ET\nfrom ultralytics import YOLO\nfrom collections import defaultdict\n\n# --- 1. PATH CONFIGURATION ---\n# Ensure these paths match your Kaggle input structure\nBASE_PATH = '/kaggle/input/vehic-ped-intuition'\nXML_PATH = '/kaggle/input/attributes-label/annotations_attributes'\nMODEL_PATH = '/kaggle/input/first-phase-model/weights/best.pt'\n\n# Load the Phase 1 YOLO model\nyolo = YOLO(MODEL_PATH)\n\ndef enhance_image(img):\n    \"\"\"\n    Applies CLAHE (Contrast Limited Adaptive Histogram Equalization) \n    to improve visibility in low-light or night scenes.\n    \"\"\"\n    if img is None: return None\n    # Convert BGR to LAB to process Luminance (L) independently\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    \n    # Create CLAHE object (clipLimit handles contrast, tileGridSize handles local areas)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    cl = clahe.apply(l)\n    \n    # Merge back and convert to BGR for YOLO processing\n    return cv2.cvtColor(cv2.merge((cl,a,b)), cv2.COLOR_LAB2BGR)\n\ndef process_split(split_name):\n    \"\"\"\n    Processes a full data split (train, val, or test), performing tracking,\n    handshaking with XML metadata, and feature engineering.\n    \"\"\"\n    print(f\"\\nüìÇ PROCESSING SPLIT: {split_name.upper()}...\")\n    \n    img_dir = os.path.join(BASE_PATH, 'images', split_name)\n    if not os.path.exists(img_dir):\n        print(f\"‚ö†Ô∏è Directory not found: {img_dir}\")\n        return\n\n    all_files = os.listdir(img_dir)\n    # Extract unique video IDs (e.g., from video_0161_f000.jpg -> 0161)\n    video_ids = sorted(list(set([f.split('_')[1] for f in all_files if '_' in f])))\n    split_rows = []\n\n    for v_id in video_ids:\n        xml_file = os.path.join(XML_PATH, f\"video_{v_id}_attributes.xml\")\n        if not os.path.exists(xml_file): continue\n        \n        # --- PHASE 2 METADATA LOADING ---\n        peds_meta = {}\n        tree = ET.parse(xml_file)\n        for p in tree.getroot().findall('pedestrian'):\n            peds_meta[p.get('id')] = {\n                'dp': int(p.get('decision_point')), \n                'crossing': int(p.get('crossing'))\n            }\n\n        # Filter and sort frames belonging to the current video\n        v_frames = sorted([os.path.join(img_dir, f) for f in all_files if f\"video_{v_id}\" in f])\n        \n        processed_tracks = defaultdict(list)\n        id_map = {} # Maps YOLO Track IDs to XML Pedestrian IDs\n        assigned_xml_ids = set()\n\n        # --- INFERENCE & TRACKING LOOP ---\n        for fidx, path in enumerate(v_frames[:100]): # Processing first 100 frames for sequence stability\n            img = cv2.imread(path)\n            img_enh = enhance_image(img) # Apply night-vision enhancement\n            \n            # Perform Tracking: persist=True maintains IDs across frames\n            results = yolo.track(img_enh, persist=True, conf=0.28, classes=[0], verbose=False)[0]\n            \n            if results.boxes.id is not None:\n                t_boxes = results.boxes.xyxy.cpu().numpy()\n                t_ids = results.boxes.id.int().cpu().numpy()\n                \n                for tb, tid in zip(t_boxes, t_ids):\n                    # XML Handshake: Assign YOLO ID to XML ID if not already mapped\n                    if tid not in id_map:\n                        for xid in peds_meta.keys():\n                            if xid not in assigned_xml_ids:\n                                id_map[tid] = xid\n                                assigned_xml_ids.add(xid)\n                                break\n                                \n                    if tid in id_map:\n                        current_xid = id_map[tid]\n                        # Store frame data for sequence building\n                        processed_tracks[current_xid].append({\n                            'frame': fidx, \n                            'bbox': tb, \n                            'before_dp': fidx <= peds_meta[current_xid]['dp']\n                        })\n\n        # --- FEATURE ENGINEERING ---\n        for xid, frames_list in processed_tracks.items():\n            # Filter: We only keep sequences (streaks) of 5+ frames for the LSTM\n            if len(frames_list) < 5: continue \n            \n            for i in range(len(frames_list)):\n                curr = frames_list[i]\n                b = curr['bbox']\n                # Calculate center coordinates and dimensions\n                cx, cy, w, h = (b[0]+b[2])/2, (b[1]+b[3])/2, b[2]-b[0], b[3]-b[1]\n                \n                # Kinetic features: Velocity and Scale Change\n                vel_x, vel_y, d_area = (0, 0, 1)\n                if i > 0:\n                    p_b = frames_list[i-1]['bbox']\n                    # Velocity = displacement between current and previous frame\n                    vel_x = cx - (p_b[0]+p_b[2])/2\n                    vel_y = cy - (p_b[1]+p_b[3])/2\n                    # Delta Area = Change in bounding box size (approaching/receding)\n                    d_area = (w*h) / ((p_b[2]-p_b[0])*(p_b[3]-p_b[1]) + 1e-6)\n                \n                split_rows.append({\n                    'video_id': v_id, \n                    'ped_id': xid, \n                    'frame': curr['frame'],\n                    'x': cx, 'y': cy, \n                    'vel_x': vel_x, 'vel_y': vel_y,\n                    'delta_area': d_area, \n                    'aspect_ratio': w/h,\n                    'before_dp': int(curr['before_dp']), # Vital for intention prediction\n                    'label': peds_meta[xid]['crossing']   # Target variable (1/0)\n                })\n    \n    # Save results to a CSV file for Phase 2 training\n    if split_rows:\n        df = pd.DataFrame(split_rows)\n        output_name = f\"master_{split_name}_dataset.csv\"\n        df.to_csv(output_name, index=False)\n        print(f\"‚úÖ Saved: {output_name} ({len(df)} rows)\")\n    else:\n        print(f\"‚ùå No data generated for split: {split_name}\")\n\n# --- EXECUTION ---\n# This will generate 3 CSV files: master_train_dataset, master_val_dataset, master_test_dataset\nfor split in ['train', 'val', 'test']:\n    process_split(split)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pre-Processor","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler\n\n# --- 1. CONFIGURATION ---\nSEQ_LEN = 10 \nFEATURES = ['x', 'y', 'vel_x', 'vel_y', 'delta_area', 'aspect_ratio']\nTARGET = 'label'\n\ndef create_lstm_sequences(csv_path, seq_len=10):\n    df = pd.read_csv(csv_path)\n    df = df[df[TARGET].isin([0, 1])]\n    df[FEATURES] = df[FEATURES].fillna(0)\n    \n    scaler = MinMaxScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    sequences, labels = [], []\n    grouped = df.groupby(['video_id', 'ped_id'])\n    \n    for (v_id, p_id), group in grouped:\n        if len(group) >= seq_len:\n            group = group.sort_values('frame')\n            feature_data = group[FEATURES].values\n            target_value = group[TARGET].mode()[0] \n            \n            for i in range(len(group) - seq_len + 1):\n                window = feature_data[i : i + seq_len]\n                sequences.append(window)\n                labels.append(target_value)\n                \n    return np.array(sequences), np.array(labels)\n\n# --- 2. THE BALANCE FIX (Oversampling) ---\nprint(\"‚öñÔ∏è Balancing training data...\")\ndf_raw = pd.read_csv('master_train_dataset.csv')\ndf_raw = df_raw[df_raw[TARGET].isin([0, 1])]\n\n# Separate majority and minority\ndf_majority = df_raw[df_raw[TARGET] == 1]\ndf_minority = df_raw[df_raw[TARGET] == 0]\n\n# Oversample the minority (duplicate rows until it matches the majority)\ndf_minority_upsampled = df_minority.sample(len(df_majority), replace=True, random_state=42)\n\n# Combine and save as a new file\ndf_balanced = pd.concat([df_majority, df_minority_upsampled])\ndf_balanced.to_csv('balanced_train_dataset.csv', index=False)\nprint(f\"‚úÖ Balanced file created: {len(df_balanced)} rows (50/50 split)\")\n\n# --- 3. EXECUTION ---\nprint(\"‚öôÔ∏è Transforming CSVs into Sequences...\")\n\n# NOTE: We use the BALANCED file for training, but ORIGINAL files for Val/Test\nX_train, y_train = create_lstm_sequences('balanced_train_dataset.csv', SEQ_LEN)\nX_val, y_val = create_lstm_sequences('master_val_dataset.csv', SEQ_LEN)\nX_test, y_test = create_lstm_sequences('master_test_dataset.csv', SEQ_LEN)\n\nprint(f\"\\n‚úÖ Preprocessing Complete!\")\nprint(f\"Train Sequences: {X_train.shape} (Should be much larger now!)\")\nprint(f\"Validation Sequences: {X_val.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\n\n# --- Move the scaler HERE (outside) so it becomes a global variable ---\nscaler = StandardScaler()\n\ndef create_lstm_sequences(csv_path, seq_len=10, is_training=False):\n    df = pd.read_csv(csv_path)\n    df = df[df[TARGET].isin([0, 1])]\n    df[FEATURES] = df[FEATURES].fillna(0)\n    \n    # --- THE FIX: Convert to .values (NumPy) BEFORE scaling ---\n    feature_values = df[FEATURES].values \n    \n    if is_training:\n        # Scaler fits on raw numbers, no names memorized\n        scaled_values = scaler.fit_transform(feature_values)\n    else:\n        scaled_values = scaler.transform(feature_values)\n    \n    # Put the scaled values back into the dataframe structure for the groupby\n    df[FEATURES] = scaled_values\n    \n    sequences, labels = [], []\n    grouped = df.groupby(['video_id', 'ped_id'])\n    \n    for (v_id, p_id), group in grouped:\n        if len(group) >= seq_len:\n            group = group.sort_values('frame')\n            feature_data = group[FEATURES].values\n            target_value = group[TARGET].mode()[0] \n            \n            for i in range(len(group) - seq_len + 1):\n                window = feature_data[i : i + seq_len]\n                sequences.append(window)\n                labels.append(target_value)\n                \n    return np.array(sequences), np.array(labels)\n\n# --- RE-RUN YOUR DATA GENERATION ---\nX_train, y_train = create_lstm_sequences('balanced_train_dataset.csv', SEQ_LEN, is_training=True)\nX_val, y_val = create_lstm_sequences('master_val_dataset.csv', SEQ_LEN, is_training=False)\nX_test, y_test = create_lstm_sequences('master_test_dataset.csv', SEQ_LEN, is_training=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:04:58.405554Z","iopub.execute_input":"2026-01-13T21:04:58.406296Z","iopub.status.idle":"2026-01-13T21:04:58.41939Z","shell.execute_reply.started":"2026-01-13T21:04:58.40625Z","shell.execute_reply":"2026-01-13T21:04:58.418094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM Construction","metadata":{}},{"cell_type":"markdown","source":"# Data Auditory before Training\n\nIdentifying noisy features, that have been ignored in the sequences building","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef audit_csv_files(file_list):\n    print(\"üîç Auditing CSV files for 'dirty' labels...\")\n    for file in file_list:\n        df = pd.read_csv(file)\n        # Check for unique values in 'label'\n        unique_labels = df['label'].unique()\n        # Count NaNs or values outside [0, 1]\n        invalid_mask = ~df['label'].isin([0, 1])\n        invalid_count = invalid_mask.sum()\n        \n        print(f\"\\nüìÑ File: {file}\")\n        print(f\"   - Unique labels found: {unique_labels}\")\n        if invalid_count > 0:\n            print(f\"   - ‚ö†Ô∏è ALERT: Found {invalid_count} invalid rows (not 0 or 1).\")\n            # Show a sample of invalid rows if they exist\n            print(df[invalid_mask][['video_id', 'ped_id', 'label']].head())\n        else:\n            print(f\"   - ‚úÖ Data is clean (only 0 and 1).\")\n\n# Run audit on your 3 generated files\naudit_csv_files(['master_train_dataset.csv', 'master_val_dataset.csv', 'master_test_dataset.csv'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM TRAINING LOOP\n","metadata":{}},{"cell_type":"markdown","source":"# Calculating the weights","metadata":{}},{"cell_type":"code","source":"print(pd.Series(y_train).value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# --- 1. INITIALIZE DEVICE ---\n# This tells the code to use the GPU if available (faster) or the CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üíª Using device: {device}\")\n\n# --- 2. CALCULATE CLASS WEIGHTS ---\n# Stay (Class 0): 4541\n# Cross (Class 1): 578\ncount_stay = 4541\ncount_cross = 578\n\n# The pos_weight formula for BCEWithLogitsLoss:\n# pos_weight = total_negative_samples / total_positive_samples\npos_weight_value = count_stay / count_cross\nclass_weights = torch.tensor([pos_weight_value]).to(device)\n\nprint(f\"‚öñÔ∏è Scale Weight for Class 1 (Crossing): {class_weights.item():.2f}\")\n\n# --- 3. INITIALIZE LOSS FUNCTION ---\n# We pass the weights here so the LSTM knows the 'Crossing' class is the priority\ncriterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Function and Class Weights\n\nThese to prevet the AI to ignore the minority group of 0 that are not moving","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. DEVICE CONFIGURATION ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- 2. STRATIFIED DATA SPLIT ---\n# Stratify ensures the 157/1380 ratio is preserved in both sets\ntrain_df, val_df = train_test_split(\n    df_test, \n    test_size=0.2, \n    stratify=df_test['label'], \n    random_state=42\n)\n\n# --- 3. SAFETY-FIRST WEIGHT CALCULATION ---\ncounts = train_df['label'].value_counts()\nstay_count = counts[0]\ncross_count = counts[1]\n\n# LOGIC: If 'Stay' is majority, we boost 'Cross'. \n# If 'Cross' is already majority (your case), we keep weight at 1.0.\n# We NEVER set it to 0.11 because that would make the AI ignore pedestrians.\nif stay_count > cross_count:\n    pos_weight_val = stay_count / cross_count\nelse:\n    pos_weight_val = 1.0  # Keep priority high for the dangerous class\n\nclass_weights = torch.tensor([pos_weight_val]).to(device)\n\nprint(f\"üìä Dataset Split Complete:\")\nprint(f\"   - Training Samples: {stay_count} Stay, {cross_count} Cross\")\nprint(f\"‚öñÔ∏è Adjusted pos_weight for 'Cross': {pos_weight_val:.2f}\")\n\n# --- 4. MODEL INITIALIZATION ---\nmodel = IntentionLSTM(input_size=6, hidden_size=64, num_layers=2, output_size=1).to(device)\n\n# --- 5. STRATEGY: OPTIMIZER & LOSS FUNCTION ---\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n\nprint(\"\\nüõ°Ô∏è Safety Mode Active: Pedestrian crossing is prioritized.\")\nprint(\"üöÄ Ready for training.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA LOADERS","metadata":{}},{"cell_type":"code","source":"# --- 3. PREPARE THE DATA FLOW (DataLoader) ---\n# CRITICAL: We turn the Numpy arrays into PyTorch Tensors and SHUFFLE them.\n# Shuffling ensures the model doesn't see a block of '0's then a block of '1's.\ntrain_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).reshape(-1, 1))\nval_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val).reshape(-1, 1))\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n\nimport numpy as np\n\n# Convert and save the test data (and train/val if you have them in memory)\n# We use .cpu().numpy() to move them from GPU to CPU and convert them\nif 'X_test' in globals():\n    np.save('/kaggle/working/X_test.npy', X_test)\n    np.save('/kaggle/working/y_test.npy', y_test)\n    print(\"‚úÖ X_test.npy and y_test.npy saved!\")\n\nif 'X_train' in globals():\n    np.save('/kaggle/working/X_train.npy', X_train)\n    np.save('/kaggle/working/y_train.npy', y_train)\n    print(\"‚úÖ X_train.npy and y_train.npy saved!\")\n\nif 'X_val' in globals():\n    np.save('/kaggle/working/X_val.npy', X_val)\n    np.save('/kaggle/working/y_val.npy', y_val)\n    print(\"‚úÖ X_val.npy and y_val.npy saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. INITIALIZE LISTS ---\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\n# --- 2. CONFIGURATION ---\nmodel = IntentionLSTM(input_size=6, hidden_size=64, num_layers=2, output_size=1).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n\nPATIENCE = 15\nbest_val_loss = float('inf')\ncounter = 0\n\nprint(\"üöÄ Starting Training Loop...\")\n\nfor epoch in range(40):\n    # --- TRAINING PHASE ---\n    model.train()\n    running_train_loss = 0.0\n    correct_train = 0\n    \n    for batch_x, batch_y in train_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        logits = model(batch_x)\n        loss = criterion(logits, batch_y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_train_loss += loss.item()\n        # Calculate accuracy for this batch\n        predicted = (torch.sigmoid(logits) > 0.5).float()\n        correct_train += (predicted == batch_y).sum().item()\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    running_val_loss = 0.0\n    correct_val = 0\n    \n    with torch.no_grad():\n        for batch_x, batch_y in val_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            v_logits = model(batch_x)\n            v_loss = criterion(v_logits, batch_y)\n            \n            running_val_loss += v_loss.item()\n            v_predicted = (torch.sigmoid(v_logits) > 0.5).float()\n            correct_val += (v_predicted == batch_y).sum().item()\n\n    # --- CALCULATE FINAL METRICS FOR THIS EPOCH ---\n    # These names MUST match the append() lines below\n    epoch_train_loss = running_train_loss / len(train_loader)\n    epoch_val_loss = running_val_loss / len(val_loader)\n    epoch_train_acc = (correct_train / len(train_loader.dataset)) * 100\n    epoch_val_acc = (correct_val / len(val_loader.dataset)) * 100\n    \n    # Store metrics for plotting\n    train_losses.append(epoch_train_loss)\n    val_losses.append(epoch_val_loss)\n    train_accs.append(epoch_train_acc)\n    val_accs.append(epoch_val_acc)\n\n    print(f\"Epoch [{epoch+1}] | Train Acc: {epoch_train_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\")\n\n    # --- EARLY STOPPING & SAVING ---\n    if epoch_val_loss < best_val_loss:\n        best_val_loss = epoch_val_loss\n        torch.save(model.state_dict(), 'best_pedestrian_model.pth')\n        print(f\"‚≠ê New Best Model Saved! Loss: {epoch_val_loss:.4f}\")\n        counter = 0\n    else:\n        counter += 1\n        if counter >= PATIENCE:\n            print(f\"üõë Early Stopping triggered at epoch {epoch+1}\")\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Traning Graphics","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nfrom datetime import datetime\n\ndef plot_and_save_training_results(t_loss, v_loss, t_acc, v_acc):\n    \"\"\"\n    Plots training and validation metrics and saves the figure with a timestamp.\n    \"\"\"\n    epochs = range(1, len(t_loss) + 1)\n    \n    # Create the figure\n    fig = plt.figure(figsize=(15, 6))\n\n    # 1. LOSS GRAPH\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, t_loss, color='blue', linestyle='-', marker='o', label='Train Loss', linewidth=2)\n    plt.plot(epochs, v_loss, color='red', linestyle='-', marker='o', label='Val Loss', linewidth=2)\n    plt.title('Model Loss (Convergence Analysis)', fontsize=14)\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss Value')\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.7)\n\n    # 2. ACCURACY GRAPH\n    plt.subplot(1, 2, 2)\n    # Separated color='orange' from the shorthand to fix the ValueError\n    plt.plot(epochs, t_acc, color='green', linestyle='--', marker='s', label='Train Acc', markersize=6)\n    plt.plot(epochs, v_acc, color='orange', linestyle='--', marker='s', label='Val Acc', markersize=6)\n    plt.title('Model Accuracy (Generalization Analysis)', fontsize=14)\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.7)\n\n    plt.tight_layout()\n\n    # --- SAVE LOGIC ---\n    # Ensure OUTPUT_DIR is defined (e.g., OUTPUT_DIR = '/kaggle/working/perception_results')\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"training_metrics_{timestamp}.png\"\n    \n    # Using a fallback if OUTPUT_DIR isn't set globally\n    try:\n        save_path = os.path.join(OUTPUT_DIR, filename)\n    except NameError:\n        save_path = filename \n\n    plt.savefig(save_path, dpi=300) \n    print(f\"üìà Training graphs saved successfully at: {save_path}\")\n    \n    plt.show()\n\n# --- EXECUTION ---\nplot_and_save_training_results(train_losses, val_losses, train_accs, val_accs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport torch\nimport os\nfrom datetime import datetime\n\n# --- 1. DEFINE OUTPUT PATH ---\n# We define it here to prevent the NameError\nOUTPUT_DIR = '/kaggle/working/perception_results'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\n# --- 2. LOAD MODEL ---\n# Ensure your model file exists in /kaggle/working/\nmodel.load_state_dict(torch.load('best_pedestrian_model.pth'))\nmodel.to(device)\nmodel.eval()\n\nall_preds = []\nall_labels = []\n\n# --- 3. GET PREDICTIONS ---\nwith torch.no_grad():\n    for batch_x, batch_y in val_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        outputs = model(batch_x)\n        preds = (torch.sigmoid(outputs) > 0.5).float()\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch_y.cpu().numpy())\n\n# --- 4. PLOT & SAVE HEATMAP ---\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n            xticklabels=['Stay', 'Cross'], \n            yticklabels=['Stay', 'Cross'],\n            annot_kws={\"size\": 16, \"weight\": \"bold\"})\n\nplt.xlabel('AI Prediction', fontsize=12)\nplt.ylabel('Actual Reality (Label)', fontsize=12)\nplt.title('Confusion Matrix: Pedestrian Intention Analysis', fontsize=14)\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\ncm_filename = f\"confusion_matrix_{timestamp}.png\"\nsave_path_cm = os.path.join(OUTPUT_DIR, cm_filename)\n\nplt.savefig(save_path_cm, dpi=300)\nprint(f\"‚úÖ Heatmap saved as: {save_path_cm}\")\nplt.show()\n\n# --- 5. SAVE TEXT REPORT ---\nreport = classification_report(all_labels, all_preds, target_names=['Stay', 'Cross'])\nreport_filename = f\"classification_report_{timestamp}.txt\"\nsave_path_report = os.path.join(OUTPUT_DIR, report_filename)\n\nwith open(save_path_report, \"w\") as f:\n    f.write(\"--- Pedestrian Intention Classification Report ---\\n\")\n    f.write(report)\n\nprint(f\"‚úÖ Text report saved as: {save_path_report}\")\nprint(\"\\n--- Classification Report ---\\n\")\nprint(report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction Script","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef predict_pedestrian_intention(sequence_data, model, scaler, device, threshold=0.5):\n    \"\"\"\n    Predicts if a pedestrian will cross based on a sequence of 10 frames.\n    \n    Args:\n        sequence_data (np.array): Shape (10, 6) -> 10 frames of [x, y, vel_x, vel_y, delta_area, aspect_ratio]\n        model: The trained LSTM model\n        scaler: The StandardScaler used during training\n        device: 'cuda' or 'cpu'\n        threshold (float): Cutoff for classification (default 0.5)\n    \"\"\"\n    model.eval()\n    \n    # 1. Preprocess the sequence (Scaling)\n    # Scaler expects (N, 6), so we flatten and then reshape back\n    scaled_sequence = scaler.transform(sequence_data)\n    \n    # 2. Convert to Tensor and add Batch dimension (1, 10, 6)\n    input_tensor = torch.Tensor(scaled_sequence).unsqueeze(0).to(device)\n    \n    # 3. Inference\n    with torch.no_grad():\n        logits = model(input_tensor)\n        probability = torch.sigmoid(logits).item() # Convert to 0.0 - 1.0 range\n    \n    # 4. Interpret Result\n    intention = \"CROSSING\" if probability > threshold else \"STAYING\"\n    confidence = probability if intention == \"CROSSING\" else (1 - probability)\n    \n    return intention, confidence, probability\n\n# --- EXAMPLE USAGE ---\n\n# Let's pretend we have 10 frames of a pedestrian walking toward the street\n# Note: In a real scenario, you would grab these from your 'df_test'\nsample_sequence = X_test[0] # Taking the first sequence from your test set\n\n# Get prediction\nlabel, certitude, raw_prob = predict_pedestrian_intention(\n    sample_sequence, \n    model, \n    scaler, # Use the scaler instance from your preprocessing cell\n    device\n)\n\nprint(f\"--- Real-Time Prediction ---\")\nprint(f\"Predicted Action: {label}\")\nprint(f\"Confidence: {certitude:.2%} (Raw Score: {raw_prob:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:27:31.163674Z","iopub.execute_input":"2026-01-14T00:27:31.163953Z","iopub.status.idle":"2026-01-14T00:27:31.175405Z","shell.execute_reply.started":"2026-01-14T00:27:31.163928Z","shell.execute_reply":"2026-01-14T00:27:31.174662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AI's Internal Debate","metadata":{}},{"cell_type":"code","source":"# Check 10 random pedestrians from the test set\nimport random\n\nprint(f\"{'Index':<8} | {'Prediction':<10} | {'Confidence':<12} | {'Actual':<10}\")\nprint(\"-\" * 50)\n\nfor i in range(10):\n    idx = random.randint(0, len(X_test) - 1)\n    \n    # Get raw data and label\n    input_data = X_test[idx]\n    actual_label = y_test[idx]\n    \n    # Inference\n    input_tensor = torch.Tensor(input_data).unsqueeze(0).to(device)\n    with torch.no_grad():\n        prob = torch.sigmoid(model(input_tensor)).item()\n    \n    pred_label = \"CROSS\" if prob > 0.5 else \"STAY\"\n    truth = \"CROSS\" if actual_label == 1 else \"STAY\"\n    conf = prob if prob > 0.5 else (1 - prob)\n    \n    # Color-coded logic (symbolic)\n    status = \"‚úÖ\" if pred_label == truth else \"‚ùå\"\n    \n    print(f\"{idx:<8} | {pred_label:<10} | {conf:<12.2%} | {truth:<10} {status}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:27:38.067951Z","iopub.execute_input":"2026-01-14T00:27:38.068238Z","iopub.status.idle":"2026-01-14T00:27:38.077139Z","shell.execute_reply.started":"2026-01-14T00:27:38.068209Z","shell.execute_reply":"2026-01-14T00:27:38.076221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OpenCV Visualization\n\n\"Serialization and Deployment Phase.\"","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Estamos usando: {device}\") # DEBE DECIR 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:44:54.503543Z","iopub.execute_input":"2026-01-13T20:44:54.503742Z","iopub.status.idle":"2026-01-13T20:44:58.328792Z","shell.execute_reply.started":"2026-01-13T20:44:54.503722Z","shell.execute_reply":"2026-01-13T20:44:58.327926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport joblib\nimport os\n\n\n# --- STEP A: SAVE ---\ntorch.save(model.state_dict(), 'lstm_intention_model.pth')\njoblib.dump(scaler, 'data_scaler.pkl')\n\n# --- STEP B: VERIFY ---\nif os.path.exists('lstm_intention_model.pth') and os.path.exists('data_scaler.pkl'):\n    print(\"‚úÖ Success! Files are saved and visible.\")\n    print(f\"Model size: {os.path.getsize('lstm_intention_model.pth') / 1024:.2f} KB\")\nelse:\n    print(\"‚ùå Error: Files were not saved correctly.\")\n\n# --- STEP C: LOAD ---\n# Now we load them back to be 100% sure they work\nmodel.load_state_dict(torch.load('lstm_intention_model.pth'))\nscaler = joblib.load('data_scaler.pkl')\nmodel.eval()\nprint(\"üß† Model and Scaler are now loaded in memory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:53:37.811049Z","iopub.execute_input":"2026-01-13T20:53:37.811668Z","iopub.status.idle":"2026-01-13T20:53:37.821877Z","shell.execute_reply.started":"2026-01-13T20:53:37.811635Z","shell.execute_reply":"2026-01-13T20:53:37.821206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Cargar el DataFrame de prueba (antes de convertirlo a secuencias LSTM)\ndf_test = pd.read_csv('master_test_dataset.csv') \n\n# Aseg√∫rate de que los nombres de las columnas coincidan con los que usa la funci√≥n\n# (video_id, frame, x, y, w, h, etc.)\nprint(df_test.head())\nprint(f\"Total de filas en df_test: {len(df_test)}\")\nprint(f\"Total de secuencias en X_test: {len(X_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:04:48.757548Z","iopub.execute_input":"2026-01-13T23:04:48.757859Z","iopub.status.idle":"2026-01-13T23:04:48.767916Z","shell.execute_reply.started":"2026-01-13T23:04:48.757831Z","shell.execute_reply":"2026-01-13T23:04:48.766936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport torch\nimport matplotlib.pyplot as plt\n\ndef draw_pedestrian_logic(img, box, prob):\n    \"\"\"\n    Draws a bounding box and prediction text on the image.\n    Green = Stay, Red = Cross\n    \"\"\"\n    x, y, w, h = box\n    \n    # Threshold: If prob > 0.5, we predict 'CROSS'\n    if prob > 0.5:\n        color = (0, 0, 255) # Red in BGR\n        label = f\"CROSSING ({prob:.1%})\"\n    else:\n        color = (0, 255, 0) # Green in BGR\n        label = f\"STAYING ({1-prob:.1%})\"\n        \n    # Draw the rectangle\n    cv2.rectangle(img, (x, y), (x + w, y + h), color, 3)\n    \n    # Draw a background label for better readability\n    cv2.rectangle(img, (x, y - 35), (x + 220, y), color, -1)\n    cv2.putText(img, label, (x + 5, y - 10), \n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n    \n    return img\n\n# Configuration for Kaggle environment\nPATH_TO_IMAGES = '/kaggle/input/vehic-ped-intuition/images/test/' \n\nimport numpy as np\nimport pandas as pd\n\n# Cargar las secuencias (X)\nX_test = np.load('/kaggle/input/phase-3-dataset/X_test.npy')\n\n# Cargar el CSV (df_test) para tener las etiquetas y nombres de archivos\n# Aseg√∫rate de que la ruta sea la correcta para tu archivo CSV de test\ndf_test = pd.read_csv('/kaggle/input/phase-3-dataset/master_test_dataset.csv') \n\nprint(f\"‚úÖ X_test cargado con forma: {X_test.shape}\")\nprint(f\"‚úÖ df_test cargado con {len(df_test)} filas\")\n\ndef visualize_real_prediction(sequence_idx, X_data, df_original):\n    \"\"\"\n    Visualizes the LSTM prediction on actual video frames.\n    Maps the processed sequence back to the specific image file.\n    \"\"\"\n    model.eval()\n    \n    # 1. Retrieve metadata from the final frame of the 10-frame sequence\n    # Since X_test[0] represents frames 0-9, we look at index + 9\n    final_info = df_original.iloc[sequence_idx + 9]\n    \n    # 2. Format Video ID (e.g., 8 -> video_0008)\n    v_id = int(final_info['video_id'])\n    video_folder = f\"video_{v_id:04d}\"\n    \n    # 3. FREQUENCY ADJUSTMENT:\n    # Our CSV uses a relative index (0, 1, 2...), but the files are saved \n    # every 5 frames (0, 5, 10...). We multiply by 5 to find the real file.\n    csv_frame = int(final_info['frame'])\n    actual_frame = csv_frame * 5 \n    \n    file_name = f\"{video_folder}_f{actual_frame:04d}.jpg\"\n    full_path = os.path.join(PATH_TO_IMAGES, file_name)\n    \n    # 4. Load the image\n    img = cv2.imread(full_path)\n    if img is None:\n        print(f\"‚ùå File not found: {full_path}\")\n        print(\"üí° Tip: Check if the video frames start at f0000 or f0005\")\n        return\n\n    # 5. Model Inference\n    # Extract the sequence and convert to tensor for the LSTM\n    sequence = X_data[sequence_idx]\n    input_tensor = torch.Tensor(sequence).unsqueeze(0).to(device)\n    with torch.no_grad():\n        # Apply Sigmoid to get a probability between 0 and 1\n        probability = torch.sigmoid(model(input_tensor)).item()\n\n    # 6. Computer Vision Drawing\n    # Get coordinates and draw the prediction box\n    x_coord = int(final_info['x'])\n    y_coord = int(final_info['y'])\n    bounding_box = [x_coord - 30, y_coord - 100, 60, 130] \n    final_img = draw_pedestrian_logic(img, bounding_box, probability)\n\n    # 7. Display Result\n    plt.figure(figsize=(12, 8))\n    plt.imshow(cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB))\n    \n    ground_truth = \"CROSS\" if final_info['label'] == 1 else \"STAY\"\n    plt.title(f\"Real-Time Visualization | CSV Frame: {csv_frame} -> File: f{actual_frame:04d}\\nAI Confidence: {probability:.2%} | Ground Truth: {ground_truth}\")\n    plt.axis('off')\n    plt.show()\n\n# --- EXECUTE ---\nvisualize_real_prediction(0, X_test, df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:04:37.932036Z","iopub.execute_input":"2026-01-13T22:04:37.93236Z","iopub.status.idle":"2026-01-13T22:04:38.268113Z","shell.execute_reply.started":"2026-01-13T22:04:37.93233Z","shell.execute_reply":"2026-01-13T22:04:38.267348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport torch\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Define the output directory in Kaggle's working space\nOUTPUT_DIR = '/kaggle/working/perception_results'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\n# Fix: Define the missing path variable\nIMAGES_BASE_PATH = '/kaggle/input/vehic-ped-intuition/images/test/' \n\n# Double-check that your drawing function is also in memory\n# If you get an error for 'draw_pedestrian_logic', run that function definition again.\n\ndef run_random_perception_test_and_save(num_frames, X_data, original_df):\n    \"\"\"\n    Selects a random video, predicts intention, and saves the output with a timestamp.\n    \"\"\"\n    # 1. Random Selection Logic\n    unique_videos = original_df['video_id'].unique()\n    random_video = random.choice(unique_videos)\n    video_indices = original_df[original_df['video_id'] == random_video].index\n    start_idx_in_x = max(0, video_indices[0]) \n    \n    # 2. Generate Timestamp for the filename\n    # Format: YYYYMMDD_HHMMSS (e.g., 20260113_143005)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    save_name = f\"video_{random_video:04d}_{timestamp}.png\"\n    save_path = os.path.join(OUTPUT_DIR, save_name)\n\n    # 3. Visualization Pipeline\n    plt.figure(figsize=(20, 10))\n    \n    for i in range(num_frames):\n        current_idx = start_idx_in_x + i\n        if current_idx >= len(X_data): break\n        \n        metadata = original_df.iloc[current_idx + 9]\n        video_id = int(metadata['video_id'])\n        frame_number = int(metadata['frame']) * 5 \n        \n        # Construct Image Path\n        img_name = f\"video_{video_id:04d}_f{frame_number:04d}.jpg\"\n        full_path = os.path.join(IMAGES_BASE_PATH, img_name)\n        \n        frame = cv2.imread(full_path)\n        if frame is None: continue\n        \n        # LSTM Inference\n        sequence = torch.Tensor(X_data[current_idx]).unsqueeze(0).to(device)\n        with torch.no_grad():\n            prediction_prob = torch.sigmoid(model(sequence)).item()\n            \n        # Draw Bounding Box and Label\n        x, y = int(metadata['x']), int(metadata['y'])\n        bbox = [x - 30, y - 100, 60, 130] \n        processed_img = draw_pedestrian_logic(frame, bbox, prediction_prob)\n        \n        # Subplot setup\n        plt.subplot(1, num_frames, i + 1)\n        plt.imshow(cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB))\n        color = 'red' if prediction_prob > 0.5 else 'green'\n        plt.title(f\"T+{i}\\nProb: {prediction_prob:.1%}\", color=color, fontsize=12, fontweight='bold')\n        plt.axis('off')\n        \n    plt.tight_layout()\n    \n    # 4. Save and Close\n    plt.savefig(save_path)\n    print(f\"‚úÖ Successfully saved: {save_name}\")\n    plt.show()\n    plt.close() # Free up memory\n\n# --- RUN MULTIPLE TESTS ---\n# This will create 5 unique files in your output folder\nfor i in range(5):\n    run_random_perception_test_and_save(5, X_test, df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:05:03.349748Z","iopub.execute_input":"2026-01-13T22:05:03.350077Z","iopub.status.idle":"2026-01-13T22:05:08.79465Z","shell.execute_reply.started":"2026-01-13T22:05:03.350048Z","shell.execute_reply":"2026-01-13T22:05:08.794097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\n\n# 1. Re-define the Architecture (Must match your trained model)\nclass IntentionLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(IntentionLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        _, (hn, _) = self.lstm(x)\n        out = self.fc(hn[-1])\n        return out\n\n# 2. Initialize and Load the LSTM\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlstm_path = '/kaggle/input/phase-3-lstm-yolo/Phase_3_models/lstm_intention_model.pth'\n\n# Note: Adjust input_size, hidden_size, num_layers to your training config\nmodel = IntentionLSTM(input_size=6, hidden_size=64, num_layers=2).to(device)\nmodel.load_state_dict(torch.load(lstm_path, map_location=device))\nmodel.eval()\nprint(\"‚úÖ LSTM Model loaded successfully\")\n\n# 3. Initialize YOLO\nyolo_model = YOLO('yolo11n.pt') \nprint(\"‚úÖ YOLO Model initialized\")\n\n# 4. Run the Pipeline\nimport cv2\nimport torch\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\n\ndef process_image_folder(folder_path, output_path, yolo_model, lstm_model, scaler):\n    # 1. Get and sort images (to ensure they are in temporal order)\n    images = sorted(glob.glob(os.path.join(folder_path, \"*.jpg\"))) # or .png\n    if not images:\n        print(f\"‚ùå No images found in {folder_path}\")\n        return\n\n    # Read first image to get dimensions\n    first_frame = cv2.imread(images[0])\n    height, width, _ = first_frame.shape\n    \n    # Setup VideoWriter to save the results as a video\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 10, (width, height)) # 10 FPS for sequence\n\n    history = {} \n    print(f\"üé¨ Processing {len(images)} frames from folder...\")\n\n    for img_path in tqdm(images):\n        frame = cv2.imread(img_path)\n        \n        # 2. YOLO Tracking\n        # Note: We use 'persist=True' to keep the IDs across separate images\n        results = yolo_model.track(frame, persist=True, classes=[0], verbose=False)\n\n        if results[0].boxes.id is not None:\n            boxes = results[0].boxes.xywh.cpu().numpy()\n            ids = results[0].boxes.id.cpu().numpy().astype(int)\n\n            for box, id in zip(boxes, ids):\n                x, y, w, h = box\n                # Features must match your training (x, y, w, h, ratio, dummy)\n                features = [x, y, w, h, w/h, 1.0] \n\n                if id not in history: history[id] = []\n                history[id].append(features)\n                \n                # Maintain the same window size as your training (e.g., 10 frames)\n                if len(history[id]) > 10: history[id].pop(0)\n\n                if len(history[id]) == 10:\n                    seq_scaled = scaler.transform(np.array(history[id]))\n                    input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)\n                    \n                    with torch.no_grad():\n                        # LSTM Prediction\n                        prob = torch.sigmoid(lstm_model(input_tensor)).item()\n\n                    # 3. Visual Feedback\n                    color = (0, 0, 255) if prob > 0.5 else (0, 255, 0)\n                    label = \"CROSSING\" if prob > 0.5 else \"STAYING\"\n                    x1, y1, x2, y2 = int(x-w/2), int(y-h/2), int(x+w/2), int(y+h/2)\n                    \n                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n                    cv2.putText(frame, f\"ID:{id} {label} {prob:.1%}\", (x1, y1-15), \n                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n        out.write(frame)\n\n    out.release()\n    print(f\"‚úÖ Video saved to: {output_path}\")\n\n# --- EXECUTION ---\n# Update this folder path to one of your test frame folders\nIMAGE_FOLDER = \"/kaggle/input/vehic-ped-intuition/images/test\" \nOUTPUT_VIDEO = \"/kaggle/working/perception_results/demo_from_frames.mp4\"\n\nprocess_image_folder(IMAGE_FOLDER, OUTPUT_VIDEO, yolo_model, model, scaler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:52:51.269697Z","iopub.execute_input":"2026-01-13T20:52:51.270029Z","iopub.status.idle":"2026-01-13T20:52:56.19393Z","shell.execute_reply.started":"2026-01-13T20:52:51.269998Z","shell.execute_reply":"2026-01-13T20:52:56.192899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\n\ndef process_video_realtime(folder_path, video_id, output_path, yolo_model, lstm_model, scaler):\n    search_pattern = os.path.join(folder_path, f\"{video_id}_*.jpg\")\n    all_frames = sorted(glob.glob(search_pattern))\n    \n    if not all_frames: \n        print(f\"‚ùå No se encontraron frames para {video_id}\")\n        return\n\n    sample_img = cv2.imread(all_frames[0])\n    h, w, _ = sample_img.shape\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 20, (w, h))\n\n    # Diccionario para guardar la trayectoria de cada peat√≥n\n    history = {} \n    \n    for frame_idx, img_path in enumerate(tqdm(all_frames, desc=f\"Video {video_id}\")):\n        frame = cv2.imread(img_path)\n        if frame is None: continue\n\n        results = yolo_model.track(frame, persist=True, classes=[0], conf=0.2, verbose=False, device=device)\n\n        if results[0].boxes.id is not None:\n            boxes = results[0].boxes.xywh.cpu().numpy()\n            ids = results[0].boxes.id.cpu().numpy().astype(int)\n\n            for box, id_ped in zip(boxes, ids):\n                xc, yc, wb, hb = box\n                current_features = [xc, yc, wb, hb, wb/hb, 1.0]\n\n                if id_ped not in history: \n                    history[id_ped] = []\n                \n                # Agregamos CADA frame para mantener la fluidez del movimiento\n                history[id_ped].append(current_features)\n                \n                # Mantenemos solo los √∫ltimos 10 frames\n                if len(history[id_ped]) > 10: \n                    history[id_ped].pop(0)\n\n                # --- VALORES POR DEFECTO (Para evitar el NameError) ---\n                color = (0, 255, 0) # Verde (Staying)\n                label = \"WAITING...\" # Mientras recolecta los 10 frames iniciales\n                \n                # --- PREDICCI√ìN CON LSTM ---\n                if len(history[id_ped]) == 10:\n                    seq_array = np.array(history[id_ped])\n                    seq_scaled = scaler.transform(seq_array)\n                    input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)\n                    \n                    with torch.no_grad():\n                        prob = torch.sigmoid(lstm_model(input_tensor)).item()\n                    \n                    # Umbral de sensibilidad 0.3\n                    if prob > 0.25:\n                        color = (0, 0, 255) # Rojo\n                        label = f\"CROSSING {prob:.0%}\"\n                    else:\n                        color = (0, 255, 0) # Verde\n                        label = f\"STAYING {prob:.0%}\"\n\n                # --- DIBUJAR EN EL FRAME ---\n                x1, y1 = int(xc - wb/2), int(yc - hb/2)\n                x2, y2 = int(xc + wb/2), int(yc + hb/2)\n                \n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                # Fondo para el texto\n                cv2.rectangle(frame, (x1, y1 - 20), (x1 + 130, y1), color, -1)\n                cv2.putText(frame, f\"ID:{id_ped} {label}\", (x1 + 2, y1 - 5), \n                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n\n        out.write(frame)\n    \n    out.release()\n    print(f\"‚úÖ Video generado con √©xito en: {output_path}\")\n\n# --- EJECUCI√ìN ---\nTARGET = \"video_0039\" \nOUTPUT = f\"/kaggle/working/perception_results/demo_{TARGET}.mp4\"\n\nprocess_video_realtime(INPUT_FOLDER, TARGET, OUTPUT, yolo_model, model, scaler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:05:37.874929Z","iopub.execute_input":"2026-01-13T23:05:37.875442Z","iopub.status.idle":"2026-01-13T23:05:40.778176Z","shell.execute_reply.started":"2026-01-13T23:05:37.87541Z","shell.execute_reply":"2026-01-13T23:05:40.777382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initializing Models for Pipeline YOLO+LSTM and CV","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\n\n# 1. Re-define the Architecture (Must match your trained model)\nclass IntentionLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(IntentionLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        _, (hn, _) = self.lstm(x)\n        out = self.fc(hn[-1])\n        return out\n\n# 2. Initialize and Load the LSTM\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlstm_path = '/kaggle/input/phase-3-lstm-yolo/Phase_3_models/lstm_intention_model.pth'\n\n# Note: Adjust input_size, hidden_size, num_layers to your training config\nmodel = IntentionLSTM(input_size=6, hidden_size=64, num_layers=2).to(device)\nmodel.load_state_dict(torch.load(lstm_path, map_location=device))\nmodel.eval()\nprint(\"‚úÖ LSTM Model loaded successfully\")\n\n# 3. Initialize YOLO\nyolo_model = YOLO('yolo11n.pt') \nprint(\"‚úÖ YOLO Model initialized\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Cargar tus datos de entrenamiento\nX_train = np.load('/kaggle/input/phase-3-dataset/X_train.npy')\n\n# 2. Reshape a 2D para el scaler (de [N, 10, 6] a [N*10, 6])\n# Esto asume que tus datos tienen 6 columnas de caracter√≠sticas\nX_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n\n# 3. Inicializar y ajustar el scaler\nscaler = StandardScaler()\nscaler.fit(X_train_reshaped)\n\nprint(f\"‚úÖ Scaler ajustado con √©xito usando X_train de forma {X_train.shape}\")\nprint(f\"üìä Media calculada para xc: {scaler.mean_[0]:.2f}, yc: {scaler.mean_[1]:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:23:17.392907Z","iopub.execute_input":"2026-01-13T21:23:17.393472Z","iopub.status.idle":"2026-01-13T21:23:17.418578Z","shell.execute_reply.started":"2026-01-13T21:23:17.393439Z","shell.execute_reply":"2026-01-13T21:23:17.417854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\n\ndef process_video_realtime(folder_path, video_id, output_path, yolo_model, lstm_model, scaler):\n    \"\"\"\n    Processes video frames, tracks pedestrians with YOLO, \n    and predicts intention (Crossing/Staying) using an LSTM model.\n    \"\"\"\n    # 1. Setup Frame Paths\n    search_pattern = os.path.join(folder_path, f\"{video_id}_*.jpg\")\n    all_frames = sorted(glob.glob(search_pattern))\n    \n    if not all_frames: \n        print(f\"‚ùå No frames found for {video_id}\")\n        return\n\n    # 2. Initialize Video Writer\n    sample_img = cv2.imread(all_frames[0])\n    h, w, _ = sample_img.shape\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 20, (w, h))\n\n    # 3. History Dictionary for Temporal Tracking\n    history = {} \n    \n    for frame_idx, img_path in enumerate(tqdm(all_frames, desc=f\"Processing {video_id}\")):\n        frame = cv2.imread(img_path)\n        if frame is None: continue\n\n        # 4. YOLO Tracking (Pedestrians only: class 0)\n        results = yolo_model.track(frame, persist=True, classes=[0], conf=0.2, verbose=False, device=device)\n\n        if results[0].boxes.id is not None:\n            boxes = results[0].boxes.xywh.cpu().numpy()\n            ids = results[0].boxes.id.cpu().numpy().astype(int)\n\n            for box, id_ped in zip(boxes, ids):\n                xc, yc, wb, hb = box\n                # Feature vector: [xc, yc, w, h, ratio, constant]\n                current_features = [xc, yc, wb, hb, wb/hb, 1.0]\n\n                if id_ped not in history: \n                    history[id_ped] = []\n                \n                # Append current frame data to history\n                history[id_ped].append(current_features)\n                \n                # Keep only the last 10 frames (LSTM window)\n                if len(history[id_ped]) > 10: \n                    history[id_ped].pop(0)\n\n                # --- DEFAULT VALUES (Staying State) ---\n                color = (0, 255, 0) # Green\n                label = \"ANALYZING...\" \n                \n                # --- LSTM INFERENCE ---\n                if len(history[id_ped]) == 10:\n                    seq_array = np.array(history[id_ped])\n                    seq_scaled = scaler.transform(seq_array)\n                    input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)\n                    \n                    with torch.no_grad():\n                        # Get probability from the model\n                        prob = torch.sigmoid(lstm_model(input_tensor)).item()\n                    \n                    # BI-LEVEL CLASSIFICATION (Threshold: 0.25)\n                    if prob > 0.25:\n                        color = (0, 0, 255) # Red for danger\n                        label = f\"CROSSING {prob:.0%}\"\n                    else:\n                        color = (0, 255, 0) # Green for safe\n                        label = f\"STAYING {prob:.0%}\"\n\n                # --- DRAWING ON FRAME ---\n                x1, y1 = int(xc - wb/2), int(yc - hb/2)\n                x2, y2 = int(xc + wb/2), int(yc + hb/2)\n                \n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                # Text Background\n                cv2.rectangle(frame, (x1, y1 - 20), (x1 + 140, y1), color, -1)\n                cv2.putText(frame, f\"ID:{id_ped} {label}\", (x1 + 2, y1 - 5), \n                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n\n        out.write(frame)\n    \n    out.release()\n    print(f\"‚úÖ Video successfully saved to: {output_path}\")\n\n# --- BATCH EXECUTION ---\nvideo_list = [\n    'video_0008', 'video_0024', 'video_0039', 'video_0042', \n    'video_0048', 'video_0054', 'video_0078', 'video_0081', \n    'video_0084', 'video_0085'\n]\n\nINPUT_DIR = \"/kaggle/input/vehic-ped-intuition/images/test\"\nOUTPUT_DIR = \"/kaggle/working/final_results\"\nif not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n\nfor vid in video_list:\n    save_path = os.path.join(OUTPUT_DIR, f\"result_{vid}.mp4\")\n    process_video_realtime(INPUT_DIR, vid, save_path, yolo_model, model, scaler)\n\n# --- ZIP AND DOWNLOAD ---\n!zip -j final_submission_videos.zip /kaggle/working/final_results/*.mp4\nprint(\"üöÄ DONE! Download 'final_submission_videos.zip' from the Output panel.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:10:21.735808Z","iopub.execute_input":"2026-01-13T23:10:21.736454Z","iopub.status.idle":"2026-01-13T23:10:41.952841Z","shell.execute_reply.started":"2026-01-13T23:10:21.736421Z","shell.execute_reply":"2026-01-13T23:10:41.952122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport glob\nfrom ultralytics import YOLO\n\n# 1. Aseg√∫rate de que el modelo est√© limpio\nyolo_model = YOLO('yolov8n.pt').to(device)\n\n# 2. Tomar una secuencia (no solo uno)\nvideo_path = \"/kaggle/input/vehic-ped-intuition/images/test/video_0039_*.jpg\"\nframes_paths = sorted(glob.glob(video_path))[:10] # Tomamos los primeros 10\n\nprint(f\"Probando secuencia de {len(frames_paths)} frames...\")\n\nfor p in frames_paths:\n    img = cv2.imread(p)\n    # Bajamos conf para forzar detecci√≥n\n    results = yolo_model.track(img, persist=True, conf=0.1, device=device)\n    \n    if results[0].boxes.id is not None:\n        ids = results[0].boxes.id.cpu().numpy()\n        print(f\"‚úÖ Frame {p.split('_')[-1]}: IDs detectados: {ids}\")\n    else:\n        print(f\"‚ùå Frame {p.split('_')[-1]}: No se detectaron IDs (solo cajas: {len(results[0].boxes)})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:50:47.522648Z","iopub.execute_input":"2026-01-13T21:50:47.523525Z","iopub.status.idle":"2026-01-13T21:50:48.009397Z","shell.execute_reply.started":"2026-01-13T21:50:47.523491Z","shell.execute_reply":"2026-01-13T21:50:48.008831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nyolo_model = YOLO('yolo11n.pt') # O el modelo que est√©s usando\nyolo_model.to(device)\n\n# Prueba r√°pida con un frame del video 0024\ntest_img_path = \"/kaggle/input/vehic-ped-intuition/images/test/video_0024_f0155.jpg\"\ntest_frame = cv2.imread(test_img_path)\n\n\nif test_frame is not None:\n    res = yolo_model.track(test_frame, persist=True, classes=[0], device=device)\n    if res[0].boxes.id is not None:\n        print(f\"‚úÖ YOLO funciona: Detectados {len(res[0].boxes.id)} peatones con ID.\")\n    else:\n        print(\"‚ùå YOLO detecta, pero NO hay IDs. Revisa el tracker.\")\nelse:\n    print(\"‚ùå No se pudo leer la imagen. Revisa la ruta.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:35:42.443298Z","iopub.execute_input":"2026-01-13T21:35:42.443639Z","iopub.status.idle":"2026-01-13T21:35:42.794174Z","shell.execute_reply.started":"2026-01-13T21:35:42.44361Z","shell.execute_reply":"2026-01-13T21:35:42.793635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comprimir solo los archivos .mp4 sin incluir las carpetas\n!zip -j final_demos_videos.zip /kaggle/working/perception_results/*.mp4\n\nprint(\"‚úÖ ¬°ZIP creado con √©xito! Solo contiene los archivos .mp4.\")\nprint(\"Descarga 'final_demos_videos.zip' desde el panel derecho (Data > Output).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:12:01.562357Z","iopub.execute_input":"2026-01-13T21:12:01.563109Z","iopub.status.idle":"2026-01-13T21:12:02.782471Z","shell.execute_reply.started":"2026-01-13T21:12:01.563076Z","shell.execute_reply":"2026-01-13T21:12:02.781609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# --- 1. PREPARE THE RESULTS DIRECTORY ---\nresults_dir = '/kaggle/working/perception_results'\nos.makedirs(results_dir, exist_ok=True)\n\n# List of specific files you mentioned to include in the Results ZIP\n# This ensures your best metrics are packaged together\nimportant_files = [\n    '/kaggle/working/training_metrics_20260113_145818.png',\n    '/kaggle/working/training_metrics_20260113_145925.png',\n    '/kaggle/working/perception_results/classification_report_20260113_150426.txt',\n    '/kaggle/working/perception_results/confusion_matrix_20260113_150426.png'\n]\n\nfor file_path in important_files:\n    if os.path.exists(file_path):\n        # Copy file to the results folder if it's not already there\n        if os.path.dirname(file_path) != results_dir:\n            shutil.copy(file_path, results_dir)\n    else:\n        print(f\"‚ö†Ô∏è File not found for ZIP: {file_path}\")\n\n# --- 2. CREATE THE ZIP FILES ---\n\n# ZIP 1: Phase 3 Results (Metrics + Report + Matrix + Prediction visuals)\nshutil.make_archive('Phase_3_Results', 'zip', results_dir)\nprint(\"‚úÖ Created: Phase_3_Results.zip\")\n\n# ZIP 2: Tracking Results (The /runs folder from YOLO/Tracking)\nruns_dir = '/kaggle/working/runs'\nif os.path.exists(runs_dir):\n    shutil.make_archive('tracking_results', 'zip', runs_dir)\n    print(\"‚úÖ Created: tracking_results.zip\")\n\n# ZIP 3: Phase 3 Models (Weights .pth, Scaler .pkl, and Config .yaml)\nmodels_temp_dir = '/kaggle/working/temp_models'\nos.makedirs(models_temp_dir, exist_ok=True)\n\nmodels_to_save = [\n    '/kaggle/working/best_pedestrian_model.pth',\n    '/kaggle/working/data_scaler.pkl',\n    '/kaggle/working/lstm_intention_model.pth',\n    '/kaggle/working/data.yaml'\n]\n\nfor model_file in models_to_save:\n    if os.path.exists(model_file):\n        shutil.copy(model_file, models_temp_dir)\n\nshutil.make_archive('Phase_3_models', 'zip', models_temp_dir)\nshutil.rmtree(models_temp_dir) # Clean up temporary folder\nprint(\"‚úÖ Created: Phase_3_models.zip\")\n\nprint(\"\\nüì¶ DONE! You can now download the 3 ZIP files from the 'Output' pane on the right.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T17:08:25.448693Z","iopub.execute_input":"2026-01-13T17:08:25.449557Z","iopub.status.idle":"2026-01-13T17:08:28.33349Z","shell.execute_reply.started":"2026-01-13T17:08:25.449522Z","shell.execute_reply":"2026-01-13T17:08:28.332834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\nimport glob\n\n# --- 1. SETUP DIRECTORIES ---\ndataset_temp_dir = '/kaggle/working/temp_dataset_master'\nos.makedirs(dataset_temp_dir, exist_ok=True)\n\n# --- 2. DEFINE FILE PATHS ---\n# CSV Files (Tabular data)\ncsv_files = [\n    '/kaggle/working/balanced_train_dataset.csv',\n    '/kaggle/working/master_test_dataset.csv',\n    '/kaggle/working/master_train_dataset.csv',\n    '/kaggle/working/master_val_dataset.csv'\n]\n\n# Config and NumPy files (The \"Actual\" LSTM inputs)\nconfig_files = ['/kaggle/working/data.yaml']\nnpy_files = glob.glob('/kaggle/working/*.npy') # This will now find the files we just saved\n\nall_dataset_files = csv_files + config_files + npy_files\n\nprint(\"üì¶ Gathering files for Dataset Master...\")\nfor file_path in all_dataset_files:\n    if os.path.exists(file_path):\n        shutil.copy(file_path, dataset_temp_dir)\n        print(f\"‚úÖ Included: {os.path.basename(file_path)}\")\n    else:\n        print(f\"‚ö†Ô∏è Warning: File not found -> {file_path}\")\n\n# --- 3. CREATE THE MASTER ZIP ---\nzip_filename = '/kaggle/working/Phase_3_Dataset_Master'\nshutil.make_archive(zip_filename, 'zip', dataset_temp_dir)\n\n# Clean up\nshutil.rmtree(dataset_temp_dir)\n\nprint(f\"\\nüöÄ SUCCESS! '{zip_filename}.zip' is ready for download.\")\nprint(\"This package is now complete with CSVs, NumPy tensors, and the YAML config.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T17:32:20.769669Z","iopub.execute_input":"2026-01-13T17:32:20.770412Z","iopub.status.idle":"2026-01-13T17:32:20.995323Z","shell.execute_reply.started":"2026-01-13T17:32:20.770381Z","shell.execute_reply":"2026-01-13T17:32:20.994783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Performance Summary\nThis project developed a Temporal Intention Predictor using a Long Short-Term Memory (LSTM) network to classify pedestrian behavior into two categories: Staying (Safe) or Crossing (Hazard). The model processes 10-frame sequences of spatial and kinematic data ($x, y, v_x, v_y, \\text{area delta, aspect ratio}$) to predict future intent.\n\nAfter addressing severe class imbalance through Minority Class Oversampling and stabilizing gradients with Standard Scaling, the model achieved the following results on the unseen test set:\n\nMetric,Value,Interpretation\nOverall Accuracy,84%,High reliability across the entire dataset.\nCrossing Recall,86%,Successfully identifies the vast majority of pedestrians entering the roadway.\nCrossing Precision,96%,\"Extremely low \"\"False Alarm\"\" rate for crossing events.\"\nStaying Recall,60%,\"Significant Achievement: Captured the majority of the minority \"\"staying\"\" class.\"\n\n**Vehicular Technology Insights (Failure Mode Analysis)**\n\nWhile the model shows high confidence (up to 99.9%) on clear trajectories, the Failure Analysis of the 16% error rate reveals critical insights for autonomous vehicle (AV) deployment:\n\nTemporal Latency: Incorrect predictions (e.g., aIndex 291) often occur when a pedestrian's transition from static to active movement is faster than the 10-frame observation window (approx. 0.3‚Äì0.5 seconds).\n\nSafety Thresholding: To mitigate the risk of \"False Negatives\" (predicting Stay when they Cross), a safety-critical system should implement a Non-Symmetric Threshold.\n\nExample: Trigger braking at 30% Crossing Probability but only resume acceleration at 10% Probability.","metadata":{}},{"cell_type":"markdown","source":"# System Architecture for Deployment\nThe system is serialized into two lightweight components for real-time edge computing (e.g., NVIDIA Jetson):\n\nlstm_intention_model.pth: A 2-layer LSTM state-dict (approx. 150 KB).\n\ndata_scaler.pkl: A Scikit-Learn StandardScaler object to ensure input consistency.","metadata":{}},{"cell_type":"code","source":"# Final Performance Note\nprint(\"--- Final Model Statistics ---\")\nprint(f\"Validation Accuracy: {max(val_accs):.2f}%\")\nprint(f\"Best Loss reached at Epoch: 4\")\nprint(\"Status: DEPLOYMENT READY\")\n\n# Save the summary to a text file for your report\nwith open('performance_summary.txt', 'w') as f:\n    f.write(f\"Model: LSTM Intention Predictor\\n\")\n    f.write(f\"Accuracy: {max(val_accs):.2f}%\\n\")\n    f.write(f\"Class 0 (Stay) Recall: 0.60\\n\")\n    f.write(f\"Class 1 (Cross) Recall: 0.86\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T16:32:44.506072Z","iopub.execute_input":"2026-01-13T16:32:44.506854Z","iopub.status.idle":"2026-01-13T16:32:44.51185Z","shell.execute_reply.started":"2026-01-13T16:32:44.50682Z","shell.execute_reply":"2026-01-13T16:32:44.511068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Configuration (All Thresholds in One Place)","metadata":{}},{"cell_type":"markdown","source":"RAW TRACKING\n‚Üí ‚ÄúWhat does YOLO + tracker see?‚Äù\n\nFILTERING & RELEVANCE SELECTION\n‚Üí ‚ÄúWhich pedestrians matter for intention?‚Äù\n\nDECISION & SEQUENCE PREPARATION\n‚Üí ‚ÄúWhich tracks become sequences?‚Äù","metadata":{}},{"cell_type":"code","source":"# Frames\nsplit = \"train\"\nIMG_DIR = f\"/kaggle/input/vehic-ped-intuition/images/{split}\"\n\n# YOLO model\nMODEL_PATH = \"/kaggle/input/first-phase-model/weights/best.pt\"\nCONF = 0.33\nIMGSZ = 640\nTRACKER = \"botsort.yaml\"\n\n# Filtering\nMIN_TRACK_LEN = 16\nINTENTION_CONF = 0.50\n\n# Relevance (distance proxy)\nMIN_MEDIAN_HEIGHT = 90     # px\nMIN_HEIGHT_GROWTH = 15     # px\n\n# Duplicate suppression\nDUP_IOU_THR = 0.70\n\n# Context crop\nEXPAND_RATIO = 1.8\n\n# Outputs\nOUT_DIR = \"/kaggle/working/smoking_test\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nRAW_VIDEO_PATH      = f\"{OUT_DIR}/tracking_raw.mp4\"\nFILTERED_VIDEO_PATH = f\"{OUT_DIR}/tracking_filtered.mp4\"\nDECISION_VIDEO_PATH = f\"{OUT_DIR}/tracking_decision.mp4\"\n\nCROPS_DIR = f\"{OUT_DIR}/crops\"         # crops per track and per frame\nSEQS_DIR  = f\"{OUT_DIR}/sequences\"     # sequences saved as folders\nFEAT_DIR  = f\"{OUT_DIR}/vit_features\"  # tensors .pt\nfor d in [CROPS_DIR, SEQS_DIR, FEAT_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nFPS = 10\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo = YOLO(MODEL_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Pick One Random Video (ALL Frames)","metadata":{}},{"cell_type":"code","source":"video_ids = sorted(set(f.split(\"_f\")[0] for f in os.listdir(IMG_DIR)))\nvideo_id = random.choice(video_ids)\n\nframes = sorted([\n    os.path.join(IMG_DIR, f)\n    for f in os.listdir(IMG_DIR)\n    if f.startswith(video_id)\n])\n\nprint(\"VIDEO:\", video_id)\nprint(\"TOTAL FRAMES:\", len(frames))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # Frame loading & raw tracking (NO FILTERING)","metadata":{}},{"cell_type":"markdown","source":"Frame loading & raw tracking (NO FILTERING)\n\nLoad all frames of one video (from IMG_DIR)\n\nRun YOLO + BoT-SORT on every frame\n\nTrack ALL detected pedestrians\n\nAssign track IDs\n\nDraw all bounding boxes + IDs","metadata":{}},{"cell_type":"code","source":"def draw_tracks_on_frame(img_bgr, frame_idx, tracks, color=(0,255,255), put_conf=False):\n    for tid, seq in tracks.items():\n        for (fidx, box, conf) in seq:\n            if fidx != frame_idx:\n                continue\n            x1,y1,x2,y2 = map(int, box)\n            cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 2)\n            txt = f\"ID {tid}\"\n            if put_conf:\n                txt += f\" {conf:.2f}\"\n            cv2.putText(img_bgr, txt, (x1, max(0,y1-7)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\ndef track_video_build_db_and_raw_video(frames):\n    track_db = {}\n    first = cv2.imread(frames[0])\n    H,W = first.shape[:2]\n    writer = cv2.VideoWriter(RAW_VIDEO_PATH, cv2.VideoWriter_fourcc(*\"mp4v\"), FPS, (W,H))\n\n    for fidx, frame_path in enumerate(frames):\n        img = cv2.imread(frame_path)\n        if img is None:\n            continue\n\n        r = yolo.track(img, conf=CONF, imgsz=IMGSZ, persist=True, tracker=TRACKER, verbose=False)[0]\n\n        if r.boxes is not None and r.boxes.id is not None:\n            boxes = r.boxes.xyxy.cpu().numpy()\n            ids   = r.boxes.id.cpu().numpy().astype(int)\n            confs = r.boxes.conf.cpu().numpy()\n\n            for box, tid, c in zip(boxes, ids, confs):\n                if tid == -1:\n                    continue\n                track_db.setdefault(tid, []).append((fidx, box, float(c)))\n\n            # draw from this frame's outputs directly (stronger visual)\n            for box, tid, c in zip(boxes, ids, confs):\n                if tid == -1:\n                    continue\n                x1,y1,x2,y2 = map(int, box)\n                cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,255), 2)\n                cv2.putText(img, f\"ID {tid} {c:.2f}\", (x1, max(0,y1-7)),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n\n        writer.write(img)\n\n    writer.release()\n    return track_db\n\ntrack_db = track_video_build_db_and_raw_video(frames)\nprint(\"Saved RAW tracking video:\", RAW_VIDEO_PATH)\nprint(\"RAW track count:\", len(track_db))\n\n# Show top tracks\nstats = sorted([(tid, len(seq), seq[0][0], seq[-1][0]) for tid, seq in track_db.items()],\n               key=lambda x: x[1], reverse=True)[:10]\nprint(\"Top tracks (tid,len,start,end):\", stats)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we do not touch frames yet ‚Äî we analyze tracks.\n\nFor each track ID, we compute:\n\nTrack-level metrics (from the config)\nFilters: length + relevance + confidence + duplicate suppression","metadata":{}},{"cell_type":"code","source":"def iou(a, b):\n    x1,y1 = max(a[0],b[0]), max(a[1],b[1])\n    x2,y2 = min(a[2],b[2]), min(a[3],b[3])\n    inter = max(0,x2-x1) * max(0,y2-y1)\n    areaA = max(0,(a[2]-a[0])) * max(0,(a[3]-a[1]))\n    areaB = max(0,(b[2]-b[0])) * max(0,(b[3]-b[1]))\n    return inter / (areaA + areaB - inter + 1e-6)\n\ndef bbox_heights(seq):\n    return np.array([(b[3]-b[1]) for (_, b, _) in seq])\n\ndef is_relevant(seq):\n    h = bbox_heights(seq)\n    return (np.median(h) >= MIN_MEDIAN_HEIGHT) or ((h[-1]-h[0]) >= MIN_HEIGHT_GROWTH)\n\ndef high_conf(seq, thr=0.5):\n    return float(np.mean([c for (_,_,c) in seq])) >= thr\n\ndef mean_iou_tracks(seq1, seq2):\n    n = min(len(seq1), len(seq2))\n    if n <= 2:\n        return 0.0\n    return float(np.mean([iou(seq1[i][1], seq2[i][1]) for i in range(n)]))\n\ndef suppress_duplicates(tracks):\n    kept = {}\n    tids = list(tracks.keys())\n    # keep longer tracks first\n    tids = sorted(tids, key=lambda t: len(tracks[t]), reverse=True)\n    for tid in tids:\n        dup = False\n        for kt in list(kept.keys()):\n            if mean_iou_tracks(tracks[tid], kept[kt]) > DUP_IOU_THR:\n                dup = True\n                break\n        if not dup:\n            kept[tid] = tracks[tid]\n    return kept\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Context-Aware Cropping","metadata":{}},{"cell_type":"code","source":"# 1) length\nt1 = {tid: seq for tid, seq in track_db.items() if len(seq) >= MIN_TRACK_LEN}\nprint(\"After MIN_TRACK_LEN:\", len(t1))\n\n# 2) relevance\nt2 = {tid: seq for tid, seq in t1.items() if is_relevant(seq)}\nprint(\"After relevance:\", len(t2))\n\n# 3) confidence\nt3 = {tid: seq for tid, seq in t2.items() if high_conf(seq, INTENTION_CONF)}\nprint(\"After confidence:\", len(t3))\n\n# 4) duplicate suppression\nfiltered_tracks = suppress_duplicates(t3)\nprint(\"After duplicate suppression:\", len(filtered_tracks))\n\n# show some stats\ndef track_stats(seq):\n    h = bbox_heights(seq)\n    return dict(\n        length=len(seq),\n        median_h=int(np.median(h)),\n        growth=int(h[-1]-h[0]),\n        mean_conf=float(np.mean([c for (_,_,c) in seq])),\n        start=int(seq[0][0]),\n        end=int(seq[-1][0]),\n    )\n\nfor tid in list(filtered_tracks.keys())[:5]:\n    print(\"Track\", tid, track_stats(filtered_tracks[tid]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first = cv2.imread(frames[0])\nH,W = first.shape[:2]\nwriter = cv2.VideoWriter(FILTERED_VIDEO_PATH, cv2.VideoWriter_fourcc(*\"mp4v\"), FPS, (W,H))\n\nfor fidx, frame_path in enumerate(frames):\n    img = cv2.imread(frame_path)\n    if img is None:\n        continue\n    draw_tracks_on_frame(img, fidx, filtered_tracks, color=(0,255,0), put_conf=False)\n    writer.write(img)\n\nwriter.release()\nprint(\"Saved FILTERED tracking video:\", FILTERED_VIDEO_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pick the longest, most stable track\nselected_tid = max(filtered_tracks.keys(),\n                   key=lambda t: len(filtered_tracks[t]))\n\nselected_track = filtered_tracks[selected_tid]\n\nprint(\"Selected TID:\", selected_tid)\nprint(\"Track length:\", len(selected_track))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"XML_GLOB = \"/kaggle/input/attributes-label/annotations_attributes/video_*_*.xml\"\nxml_files = sorted(glob.glob(XML_GLOB))\nprint(\"XML files found:\", len(xml_files))\n\n# Find XML candidates containing video_id string\ncandidates = [x for x in xml_files if video_id in os.path.basename(x)]\nprint(\"XML candidates for this video:\", len(candidates))\nprint(\"Example candidates:\", candidates[:3])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xml_path = candidates[0] if len(candidates) > 0 else xml_files[0]\nprint(\"Using XML:\", xml_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xml_label = \"crossing\" if ped_attrs[0][\"crossing\"] == 1 else \"not_crossing\"\nprint(\"Assigned label:\", xml_label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cut = int(0.8 * len(selected_track))\nusable_seq = selected_track[:cut]\n\nprint(\"Using frames:\", usable_seq[0][0], \"to\", usable_seq[-1][0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Building Sliding-Window Sequences","metadata":{}},{"cell_type":"code","source":"SEQ_LEN = 16\nSTRIDE = 4\n\ndef build_windows(seq, T=16, stride=4):\n    windows = []\n    for i in range(0, len(seq) - T + 1, stride):\n        windows.append(seq[i:i+T])\n    return windows\n\nwindows = build_windows(usable_seq, SEQ_LEN, STRIDE)\nprint(\"Total sequences:\", len(windows))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"======================","metadata":{}},{"cell_type":"markdown","source":"# Phase2","metadata":{}},{"cell_type":"code","source":"import torch\nimport timm\nimport os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\n# Constants\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_ROOT = '/kaggle/input/vehic-ped-intuition/images'\nLBL_ROOT = '/kaggle/input/vehic-ped-intuition/labels'\n\n# Load ViT Model (Base version with 768-dimensional features)\nprint(f\"Loading ViT on {DEVICE}...\")\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0).to(DEVICE)\nmodel.eval()\n\n# Preprocessing for ViT\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_pedestrian_features(img_path, label_path):\n    features_list = []\n    \n    # Load image\n    img = cv2.imread(img_path)\n    if img is None: return None\n    h, w, _ = img.shape\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Read YOLO labels\n    if os.path.exists(label_path):\n        with open(label_path, 'r') as f:\n            for line in f.readlines():\n                # YOLO format: cls, x_center, y_center, width, height\n                _, x, y, nw, nh = map(float, line.split())\n                \n                # Convert to pixel coordinates\n                x1 = int((x - nw/2) * w)\n                y1 = int((y - nh/2) * h)\n                x2 = int((x + nw/2) * w)\n                y2 = int((y + nh/2) * h)\n\n                # Ensure crop is within image bounds\n                crop = img_rgb[max(0, y1):min(h, y2), max(0, x1):min(w, x2)]\n                \n                if crop.size > 0:\n                    # Transform and extract features\n                    pil_img = Image.fromarray(crop)\n                    img_tensor = transform(pil_img).unsqueeze(0).to(DEVICE)\n                    \n                    with torch.no_grad():\n                        feat = model(img_tensor)\n                        features_list.append(feat.cpu().numpy().flatten())\n    \n    return features_list # Returns list of 768-dim vectors","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndef create_baseline_sequences(features_path, seq_len=10):\n    # Load your extracted ViT features\n    data = np.load(features_path) \n    \n    sequences = []\n    # Simplified: We slide a window of 'seq_len' across the frames\n    # In the final version, we will group these strictly by Video ID\n    for i in range(0, len(data) - seq_len, 5): # Step of 5 to reduce overlap\n        sequences.append(data[i : i + seq_len])\n    \n    return np.array(sequences)\n\n# Prepare Train and Val baseline data\nX_train_base = create_baseline_sequences('/kaggle/working/vit_train_features.npy')\nX_val_base = create_baseline_sequences('/kaggle/working/vit_val_features.npy')\n\n# Placeholder labels (replace with your actual intent labels)\ny_train_base = np.random.randint(0, 2, len(X_train_base))\ny_val_base = np.random.randint(0, 2, len(X_val_base))\n\nprint(f\"Baseline Train Shapes: {X_train_base.shape}\") # Goal: (N, 10, 768)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"21-","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Baseline LSTM model\nclass BaselineLSTM(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=128):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (batch, 10, 768)\n        _, (hn, _) = self.lstm(x)      # hn: (num_layers, B, hidden_dim)\n        out = self.fc(hn[-1])         # (B, 1)\n        return self.sigmoid(out)      # probability in [0,1]\n\n# Initialize model\nbaseline_model = BaselineLSTM().to(device)\n\n# Check parameters\nprint(\n    \"Trainable parameters:\",\n    sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"22- ","metadata":{}},{"cell_type":"code","source":"print(X_train_base.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass BaselineLSTM(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=128):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (batch, 10, 768)\n        _, (hn, _) = self.lstm(x)\n        out = self.fc(hn[-1])         # (B, 1)\n        return self.sigmoid(out)      # probability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_model = BaselineLSTM().to(device)\n\nprint(\n    \"Trainable parameters:\",\n    sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nimport torch\n\ntrain_loader = DataLoader(\n    TensorDataset(\n        torch.tensor(X_train_base, dtype=torch.float32),\n        torch.tensor(y_train_base, dtype=torch.float32)\n    ),\n    batch_size=32,\n    shuffle=True\n)\n\nval_loader = DataLoader(\n    TensorDataset(\n        torch.tensor(X_val_base, dtype=torch.float32),\n        torch.tensor(y_val_base, dtype=torch.float32)\n    ),\n    batch_size=32\n)\n\nprint(\"Train batches:\", len(train_loader))\nprint(\"Val batches:\", len(val_loader))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\n\noptimizer = optim.Adam(baseline_model.parameters(), lr=1e-3)\n\n# Model outputs probability (Sigmoid already applied)\ncriterion = nn.BCELoss()\n\nprint(\"Optimizer and loss are ready ‚úÖ\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TRAINING LOOP**","metadata":{}},{"cell_type":"code","source":"EPOCHS = 15\n\nfor epoch in range(EPOCHS):\n    baseline_model.train()\n    total_loss = 0.0\n\n    for batch_x, batch_y in train_loader:\n        batch_x = batch_x.to(device)\n        batch_y = batch_y.to(device).view(-1, 1)\n\n        optimizer.zero_grad()\n        preds = baseline_model(batch_x)      # probability in [0,1]\n        loss = criterion(preds, batch_y)     # BCELoss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {avg_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**VALIDATION / BASELINE ACCURACY**","metadata":{}},{"cell_type":"code","source":"baseline_model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for batch_x, batch_y in val_loader:\n        batch_x = batch_x.to(device)\n        batch_y = batch_y.to(device).view(-1, 1)\n\n        probs = baseline_model(batch_x)   # ALREADY probability\n        preds = (probs > 0.5).float()\n\n        correct += (preds == batch_y).sum().item()\n        total += batch_y.size(0)\n\naccuracy = correct / total\nprint(f\"Baseline Validation Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprint(\"y_train unique/counts:\", np.unique(y_train_base, return_counts=True))\nprint(\"y_val   unique/counts:\", np.unique(y_val_base, return_counts=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nfrom collections import defaultdict\n\nIMG_DIR = \"/kaggle/input/vehic-ped-intuition/images/train\"\n\ndef get_video_id(filename):\n    \"\"\"\n    Extract video ID from filenames like:\n    video_0051_frame_000123.jpg -> video_0051\n    \"\"\"\n    match = re.search(r\"(video_\\d+)\", filename)\n    return match.group(1) if match else None\n\n# Group frames by video\nvideo_frames = defaultdict(list)\n\nfor fname in sorted(os.listdir(IMG_DIR)):\n    if not fname.lower().endswith(\".jpg\"):\n        continue\n    vid = get_video_id(fname)\n    if vid is not None:\n        video_frames[vid].append(fname)\n\nprint(\"Total videos found:\", len(video_frames))\n\n# Show a few examples\nfor i, (vid, frames) in enumerate(video_frames.items()):\n    print(f\"{vid}: {len(frames)} frames\")\n    if i == 4:\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.2**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nSEQ_LEN = 10\n\ndef sample_frames_uniform(frames, seq_len=10):\n    \"\"\"\n    Select seq_len frames uniformly from a list of frames.\n    If video has fewer frames, pad by repeating last frame.\n    \"\"\"\n    n = len(frames)\n    \n    if n >= seq_len:\n        indices = np.linspace(0, n - 1, seq_len).astype(int)\n        return [frames[i] for i in indices]\n    else:\n        # Pad by repeating last frame\n        return frames + [frames[-1]] * (seq_len - n)\n\n\n# Test on a few videos\nfor vid in list(video_frames.keys())[:5]:\n    sampled = sample_frames_uniform(video_frames[vid], SEQ_LEN)\n    print(vid, \"‚Üí sampled frames:\", len(sampled))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.3**\nExtracting features for each video by focusing on 20 sequence of frames","metadata":{}},{"cell_type":"code","source":"!pip install -U ultralytics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport torch\n\n# ---- SETTINGS ----\nSEQ_LEN = 20\nDATA_ROOT = \"/kaggle/input/vehic-ped-intuition\"\nOUT_DIR = \"/kaggle/working/phase3_video_features\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# ---- REQUIRED: ViT model + transform + DEVICE must exist ----\n# Expecting: model, transform, DEVICE\nassert \"model\" in globals(), \"ViT model not found. Run the ViT setup cell first (timm.create_model...).\"\nassert \"transform\" in globals(), \"transform not found. Run the ViT preprocessing cell first.\"\nassert \"DEVICE\" in globals(), \"DEVICE not found. Define DEVICE = 'cuda' if available else 'cpu'.\"\n\ndef get_video_id(filename):\n    m = re.search(r\"(video_\\d+)\", filename)\n    return m.group(1) if m else None\n\ndef group_frames_by_video(img_dir):\n    d = defaultdict(list)\n    for fname in sorted(os.listdir(img_dir)):\n        if fname.lower().endswith(\".jpg\"):\n            vid = get_video_id(fname)\n            if vid:\n                d[vid].append(fname)\n    return d\n\ndef sample_frames_uniform(frames, seq_len=10):\n    n = len(frames)\n    if n >= seq_len:\n        idx = np.linspace(0, n - 1, seq_len).astype(int)\n        return [frames[i] for i in idx]\n    else:\n        return frames + [frames[-1]] * (seq_len - n)\n\ndef extract_one_frame_feature(img_path, lbl_path):\n    \"\"\"\n    Returns a single 768-dim feature for the frame using the LARGEST pedestrian bbox.\n    If no bbox -> None\n    \"\"\"\n    img = cv2.imread(img_path)\n    if img is None:\n        return None\n\n    h, w = img.shape[:2]\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    if not os.path.exists(lbl_path):\n        return None\n\n    best_crop = None\n    best_area = 0\n\n    with open(lbl_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 5:\n                continue\n\n            _, x, y, nw, nh = map(float, parts)\n\n            x1 = int((x - nw/2) * w)\n            y1 = int((y - nh/2) * h)\n            x2 = int((x + nw/2) * w)\n            y2 = int((y + nh/2) * h)\n\n            x1 = max(0, min(w-1, x1))\n            y1 = max(0, min(h-1, y1))\n            x2 = max(0, min(w-1, x2))\n            y2 = max(0, min(h-1, y2))\n\n            if x2 <= x1 or y2 <= y1:\n                continue\n\n            area = (x2 - x1) * (y2 - y1)\n            if area > best_area:\n                best_area = area\n                best_crop = img_rgb[y1:y2, x1:x2]\n\n    if best_crop is None or best_crop.size == 0:\n        return None\n\n    pil_img = Image.fromarray(best_crop)\n    img_tensor = transform(pil_img).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        feat = model(img_tensor)\n        if feat.ndim == 3:\n            feat = feat[:, 0, :]\n        feat = feat.cpu().numpy().squeeze().astype(np.float32)\n\n    return feat  # (768,)\n\ndef build_split_video_tensors(split_name):\n    img_dir = f\"{DATA_ROOT}/images/{split_name}\"\n    lbl_dir = f\"{DATA_ROOT}/labels/{split_name}\"\n\n    split_video_frames = group_frames_by_video(img_dir)\n    video_ids = sorted(split_video_frames.keys())\n\n    X = []\n    meta = []\n\n    def label_path_for(fname):\n        return os.path.join(lbl_dir, fname.rsplit(\".\", 1)[0] + \".txt\")\n\n    print(f\"\\nExtracting {split_name} videos: {len(video_ids)}\")\n    for vid in tqdm(video_ids):\n        frames = split_video_frames[vid]\n        sampled = sample_frames_uniform(frames, SEQ_LEN)\n\n        feats = []\n        last_valid = None\n\n        for fname in sampled:\n            img_path = os.path.join(img_dir, fname)\n            lbl_path = label_path_for(fname)\n\n            fvec = extract_one_frame_feature(img_path, lbl_path)\n\n            if fvec is None:\n                fvec = last_valid if last_valid is not None else np.zeros((768,), dtype=np.float32)\n            else:\n                last_valid = fvec\n\n            feats.append(fvec)\n\n        X.append(np.stack(feats, axis=0))  # (10,768)\n        meta.append(vid)\n\n    X = np.stack(X, axis=0)  # (N,10,768)\n    return X, meta\n\n# ---- BUILD VAL + TEST ----\nX_val_videos, val_video_ids = build_split_video_tensors(\"val\")\nprint(\"‚úÖ X_val_videos shape:\", X_val_videos.shape)\n\nX_test_videos, test_video_ids = build_split_video_tensors(\"test\")\nprint(\"‚úÖ X_test_videos shape:\", X_test_videos.shape)\n\n# ---- SAVE ----\nnp.save(os.path.join(OUT_DIR, \"X_val_videos.npy\"), X_val_videos)\nnp.save(os.path.join(OUT_DIR, \"X_test_videos.npy\"), X_test_videos)\nnp.save(os.path.join(OUT_DIR, \"val_video_ids.npy\"), np.array(val_video_ids))\nnp.save(os.path.join(OUT_DIR, \"test_video_ids.npy\"), np.array(test_video_ids))\n\nprint(\"\\n‚úÖ Saved to:\", OUT_DIR)\nprint(\" - X_val_videos.npy, X_test_videos.npy\")\nprint(\" - val_video_ids.npy, test_video_ids.npy\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_videos, train_video_ids = build_split_video_tensors(\"train\")\nprint(\"‚úÖ X_train_videos shape:\", X_train_videos.shape)\n\nnp.save(os.path.join(OUT_DIR, \"X_train_videos.npy\"), X_train_videos)\nnp.save(os.path.join(OUT_DIR, \"train_video_ids.npy\"), np.array(train_video_ids))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport torch\n\nSEQ_LEN = 10\nDATA_ROOT = \"/kaggle/input/vehic-ped-intuition\"\nIMG_DIR = f\"{DATA_ROOT}/images/train\"\nLBL_DIR = f\"{DATA_ROOT}/labels/train\"\n\n# --- helpers ---\ndef get_video_id(filename):\n    m = re.search(r\"(video_\\d+)\", filename)\n    return m.group(1) if m else None\n\ndef group_frames_by_video(img_dir):\n    d = defaultdict(list)\n    for fname in sorted(os.listdir(img_dir)):\n        if fname.lower().endswith(\".jpg\"):\n            vid = get_video_id(fname)\n            if vid:\n                d[vid].append(fname)\n    return d\n\ndef sample_frames_uniform(frames, seq_len=10):\n    n = len(frames)\n    if n >= seq_len:\n        idx = np.linspace(0, n - 1, seq_len).astype(int)\n        return [frames[i] for i in idx]\n    else:\n        return frames + [frames[-1]] * (seq_len - n)\n\ndef extract_one_frame_feature(img_path, lbl_path):\n    img = cv2.imread(img_path)\n    if img is None:\n        return None\n\n    h, w = img.shape[:2]\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    if not os.path.exists(lbl_path):\n        return None\n\n    best_crop, best_area = None, 0\n\n    with open(lbl_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 5:\n                continue\n\n            _, x, y, nw, nh = map(float, parts)\n\n            x1 = int((x - nw/2) * w)\n            y1 = int((y - nh/2) * h)\n            x2 = int((x + nw/2) * w)\n            y2 = int((y + nh/2) * h)\n\n            x1, y1 = max(0, x1), max(0, y1)\n            x2, y2 = min(w-1, x2), min(h-1, y2)\n\n            if x2 <= x1 or y2 <= y1:\n                continue\n\n            area = (x2 - x1) * (y2 - y1)\n            if area > best_area:\n                best_area = area\n                best_crop = img_rgb[y1:y2, x1:x2]\n\n    if best_crop is None or best_crop.size == 0:\n        return None\n\n    pil_img = Image.fromarray(best_crop)\n    img_tensor = transform(pil_img).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        feat = model(img_tensor)\n        if feat.ndim == 3:\n            feat = feat[:, 0, :]\n        feat = feat.cpu().numpy().squeeze().astype(np.float32)\n\n    return feat  # (768,)\n\n# --- build train ---\nvideo_frames = group_frames_by_video(IMG_DIR)\nvideo_ids = sorted(video_frames.keys())\n\nX_train_videos = []\ntrain_meta = []\n\nprint(\"Extracting TRAIN video features:\", len(video_ids))\n\nfor vid in tqdm(video_ids):\n    frames = video_frames[vid]\n    sampled = sample_frames_uniform(frames, SEQ_LEN)\n\n    feats, last_valid = [], None\n\n    for fname in sampled:\n        img_path = os.path.join(IMG_DIR, fname)\n        lbl_path = os.path.join(LBL_DIR, fname.rsplit(\".\", 1)[0] + \".txt\")\n\n        fvec = extract_one_frame_feature(img_path, lbl_path)\n        if fvec is None:\n            fvec = last_valid if last_valid is not None else np.zeros((768,), dtype=np.float32)\n        else:\n            last_valid = fvec\n\n        feats.append(fvec)\n\n    X_train_videos.append(np.stack(feats, axis=0))\n    train_meta.append(vid)\n\nX_train_videos = np.stack(X_train_videos, axis=0)\nprint(\"‚úÖ X_train_videos shape:\", X_train_videos.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.4**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nOUT_DIR = \"/kaggle/working/phase3_video_features\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# Save train if it exists in memory\nassert \"X_train_videos\" in globals(), \"X_train_videos not found. Run Phase 3 Step 3.2 (train extraction) first.\"\nassert \"train_meta\" in globals(), \"train_meta (train video ids) not found.\"\n\nnp.save(os.path.join(OUT_DIR, \"X_train_videos.npy\"), X_train_videos)\nnp.save(os.path.join(OUT_DIR, \"train_video_ids.npy\"), np.array(train_meta))\n\nprint(\"‚úÖ Saved train tensors too:\")\nprint(\" - X_train_videos.npy:\", X_train_videos.shape)\nprint(\" - train_video_ids.npy:\", len(train_meta))\nprint(\"Folder:\", OUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4**","metadata":{}},{"cell_type":"code","source":"# Sanity check: IDs and labels\nmissing = []\nfor vid in val_video_ids:\n    if vid not in video_to_label:\n        missing.append(vid)\n\nprint(\"Missing labels for val:\", missing[:5])\nprint(\"Total missing:\", len(missing))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. PCA , Embedding 2D**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nOUT_DIR = \"/kaggle/working/phase3_video_features\"\n\nE_train = np.load(os.path.join(OUT_DIR, \"E_train.npy\"))\nE_val   = np.load(os.path.join(OUT_DIR, \"E_val.npy\"))\nE_test  = np.load(os.path.join(OUT_DIR, \"E_test.npy\"))\n\n# Combine for visualization\nE_all = np.vstack([E_train, E_val, E_test])\nsplit_tags = ([\"train\"] * len(E_train)) + ([\"val\"] * len(E_val)) + ([\"test\"] * len(E_test))\n\npca = PCA(n_components=2, random_state=0)\nZ = pca.fit_transform(E_all)\n\nprint(\"Explained variance ratio (PC1, PC2):\", pca.explained_variance_ratio_)\n\n# Plot\nplt.figure(figsize=(7, 6))\nfor tag in [\"train\", \"val\", \"test\"]:\n    idx = [i for i, t in enumerate(split_tags) if t == tag]\n    plt.scatter(Z[idx, 0], Z[idx, 1], label=tag, alpha=0.7)\n\nplt.title(\"PCA of Video Embeddings (Phase 3)\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()\n\n# Save figure\nfig_path = os.path.join(OUT_DIR, \"pca_video_embeddings.png\")\nplt.figure(figsize=(7, 6))\nfor tag in [\"train\", \"val\", \"test\"]:\n    idx = [i for i, t in enumerate(split_tags) if t == tag]\n    plt.scatter(Z[idx, 0], Z[idx, 1], label=tag, alpha=0.7)\nplt.title(\"PCA of Video Embeddings (Phase 3)\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"‚úÖ Saved figure:\", fig_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport xml.etree.ElementTree as ET\n\nXML_DIR = \"/kaggle/input/attributes-label/annotations_attributes/\"\n\ndef parse_video_crossing_label(xml_path):\n    \"\"\"\n    Returns:\n      1  if ANY pedestrian has crossing == 1\n      0  if at least one crossing == 0 and none == 1\n      None if all are -1 or file invalid\n    \"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n    except Exception:\n        return None\n\n    crossings = []\n    for ped in root.findall(\".//pedestrian\"):\n        c = ped.attrib.get(\"crossing\", None)\n        if c is None:\n            continue\n        try:\n            crossings.append(int(c))\n        except:\n            continue\n\n    if len(crossings) == 0:\n        return None\n\n    if any(c == 1 for c in crossings):\n        return 1\n    if any(c == 0 for c in crossings):\n        return 0\n\n    return None  # all -1\n\n# Build mapping: video_id -> label\nxml_files = sorted(glob.glob(os.path.join(XML_DIR, \"video_*_attributes.xml\")))\nvideo_to_label = {}\n\nfor xp in xml_files:\n    fname = os.path.basename(xp)                  # video_0012_attributes.xml\n    vid = fname.split(\"_attributes.xml\")[0]       # video_0012\n    video_to_label[vid] = parse_video_crossing_label(xp)\n\n# Summary\nlabels = [v for v in video_to_label.values() if v is not None]\nprint(\"Total XML files:\", len(xml_files))\nprint(\"Labeled videos:\", len(labels))\nprint(\"Crossing=1:\", sum(1 for v in labels if v == 1))\nprint(\"Crossing=0:\", sum(1 for v in labels if v == 0))\nprint(\"Unknown (-1):\", sum(1 for v in video_to_label.values() if v is None))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_labels(video_ids, video_to_label):\n    X_ids, y = [], []\n    for vid in video_ids:\n        lab = video_to_label.get(vid, None)\n        if lab is None:\n            continue\n        X_ids.append(vid)\n        y.append(lab)\n    return np.array(X_ids), np.array(y, dtype=np.int64)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_ids_labeled, y_val = build_labels(val_video_ids, video_to_label)\ntest_ids_labeled, y_test = build_labels(test_video_ids, video_to_label)\n\nprint(\"Val labels:\", np.bincount(y_val))\nprint(\"Test labels:\", np.bincount(y_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_X_by_ids(X, video_ids, keep_ids):\n    id_to_idx = {vid: i for i, vid in enumerate(video_ids)}\n    idxs = [id_to_idx[vid] for vid in keep_ids]\n    return X[idxs]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_val_labeled = filter_X_by_ids(X_val_videos, val_video_ids, val_ids_labeled)\nX_test_labeled = filter_X_by_ids(X_test_videos, test_video_ids, test_ids_labeled)\n\nprint(X_val_labeled.shape, y_val.shape)\nprint(X_test_labeled.shape, y_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\n\n# Paths\nOUT_DIR = \"/kaggle/working/phase3_video_features\"\n\n# Load embeddings\nE_train = np.load(os.path.join(OUT_DIR, \"X_train_videos.npy\"))\nE_val   = np.load(os.path.join(OUT_DIR, \"X_val_videos.npy\"))\nE_test  = np.load(os.path.join(OUT_DIR, \"X_test_videos.npy\"))\n\n# Load video ids\ntrain_ids = np.load(os.path.join(OUT_DIR, \"train_video_ids.npy\"), allow_pickle=True)\nval_ids   = np.load(os.path.join(OUT_DIR, \"val_video_ids.npy\"), allow_pickle=True)\ntest_ids  = np.load(os.path.join(OUT_DIR, \"test_video_ids.npy\"), allow_pickle=True)\n\n# video_to_label MUST exist from Step 1\nassert \"video_to_label\" in globals(), \"video_to_label not found. Run Step 1 (XML parsing) first.\"\n\ndef filter_labeled(E, ids, video_to_label):\n    X, y, kept = [], [], []\n    for emb, vid in zip(E, ids):\n        vid = str(vid)\n        lab = video_to_label.get(vid, None)\n        if lab is None:\n            continue\n        X.append(emb)\n        y.append(lab)\n        kept.append(vid)\n    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64), kept\n\n# Apply filtering\nXtr, ytr, tr_ids = filter_labeled(E_train, train_ids, video_to_label)\nXva, yva, va_ids = filter_labeled(E_val,   val_ids,   video_to_label)\nXte, yte, te_ids = filter_labeled(E_test,  test_ids,  video_to_label)\n\n# Summary\nprint(\"Train:\", Xtr.shape, \"Pos:\", int((ytr==1).sum()), \"Neg:\", int((ytr==0).sum()))\nprint(\"Val:  \", Xva.shape, \"Pos:\", int((yva==1).sum()), \"Neg:\", int((yva==0).sum()))\nprint(\"Test: \", Xte.shape, \"Pos:\", int((yte==1).sum()), \"Neg:\", int((yte==0).sum()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pos = (ytr == 1).sum()\nneg = (ytr == 0).sum()\npos_weight = torch.tensor([neg / pos]).to(device)\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PHSAE 4**","metadata":{}},{"cell_type":"code","source":"import os, glob, random\nimport numpy as np\nimport xml.etree.ElementTree as ET\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    classification_report,\n    roc_auc_score\n)\n\n# ======================================================\n# 0) Reproducibility\n# ======================================================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# ======================================================\n# 1) Paths & device\n# ======================================================\nFEAT_DIR = \"/kaggle/working/phase3_video_features\"\nXML_DIR  = \"/kaggle/input/attributes-label/annotations_attributes\"\nSAVE_DIR = \"/kaggle/working/phase3_lstm_supervised\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\n# ======================================================\n# 2) Load features + video IDs\n# ======================================================\nX_train = np.load(os.path.join(FEAT_DIR, \"X_train_videos.npy\"))\nX_val   = np.load(os.path.join(FEAT_DIR, \"X_val_videos.npy\"))\nX_test  = np.load(os.path.join(FEAT_DIR, \"X_test_videos.npy\"))\n\ntrain_ids = np.load(os.path.join(FEAT_DIR, \"train_video_ids.npy\"), allow_pickle=True)\nval_ids   = np.load(os.path.join(FEAT_DIR, \"val_video_ids.npy\"),   allow_pickle=True)\ntest_ids  = np.load(os.path.join(FEAT_DIR, \"test_video_ids.npy\"),  allow_pickle=True)\n\n# ======================================================\n# 3) XML parsing ‚Üí video-level crossing label\n# ======================================================\ndef parse_crossing(xml_path):\n    try:\n        root = ET.parse(xml_path).getroot()\n    except Exception:\n        return None\n\n    vals = []\n    for ped in root.findall(\".//pedestrian\"):\n        c = ped.attrib.get(\"crossing\", None)\n        if c is not None:\n            try:\n                vals.append(int(c))\n            except:\n                pass\n\n    if not vals:\n        return None\n    if any(v == 1 for v in vals):\n        return 1\n    if any(v == 0 for v in vals):\n        return 0\n    return None\n\nvideo_to_label = {}\nfor xp in glob.glob(os.path.join(XML_DIR, \"video_*_attributes.xml\")):\n    vid = os.path.basename(xp).replace(\"_attributes.xml\", \"\")\n    video_to_label[vid] = parse_crossing(xp)\n\ndef filter_labeled(X, ids):\n    X_out, y_out = [], []\n    for x, vid in zip(X, ids):\n        lab = video_to_label.get(str(vid), None)\n        if lab is None:\n            continue\n        X_out.append(x)\n        y_out.append(lab)\n    return np.asarray(X_out, np.float32), np.asarray(y_out, np.float32)\n\nXtr, ytr = filter_labeled(X_train, train_ids)\nXva, yva = filter_labeled(X_val,   val_ids)\nXte, yte = filter_labeled(X_test,  test_ids)\n\nprint(\"Train:\", Xtr.shape, \"Pos:\", int((ytr==1).sum()), \"Neg:\", int((ytr==0).sum()))\nprint(\"Val:  \", Xva.shape)\nprint(\"Test: \", Xte.shape)\n\n# ======================================================\n# 4) DataLoaders\n# ======================================================\ntrain_loader = DataLoader(\n    TensorDataset(torch.tensor(Xtr), torch.tensor(ytr).view(-1,1)),\n    batch_size=32, shuffle=True\n)\nval_loader = DataLoader(\n    TensorDataset(torch.tensor(Xva), torch.tensor(yva).view(-1,1)),\n    batch_size=32\n)\ntest_loader = DataLoader(\n    TensorDataset(torch.tensor(Xte), torch.tensor(yte).view(-1,1)),\n    batch_size=32\n)\n\n# ======================================================\n# 5) LSTM Classifier\n# ======================================================\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=128, dropout=0.4):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        last = out[:, -1, :]\n        last = self.dropout(last)\n        return self.fc(last)\n\nmodel = LSTMClassifier().to(device)\n\n# ======================================================\n# 6) Loss + Optimizer (imbalance handled)\n# ======================================================\npos = (ytr == 1).sum()\nneg = (ytr == 0).sum()\npos_weight = torch.tensor([neg / max(pos, 1)], device=device)\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nprint(\"pos_weight:\", float(pos_weight))\n\n# ======================================================\n# 7) Training + Early Stopping + METRIC LOGGING\n# ======================================================\n# ======================================================\n# 7) Training + Early Stopping + METRIC LOGGING\n# ======================================================\nEPOCHS = 20\nPATIENCE = 5\nbest_auc = -1.0\npatience_ctr = 0\nbest_state = None\n\ntrain_losses = []\nval_losses   = []\nval_aucs     = []\n\ndef eval_val_metrics(loader):\n    model.eval()\n    total_loss = 0.0\n    ys, ps = [], []\n\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            total_loss += loss.item()\n\n            probs = torch.sigmoid(logits)\n            ys.append(yb.cpu().numpy())\n            ps.append(probs.cpu().numpy())\n\n    y = np.vstack(ys).ravel()\n    p = np.vstack(ps).ravel()\n    avg_loss = total_loss / len(loader)\n    auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else None\n    return avg_loss, auc\n\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(xb), yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    train_loss = total_loss / len(train_loader)\n    val_loss, val_auc = eval_val_metrics(val_loader)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    val_aucs.append(val_auc)\n\n    print(\n        f\"Epoch {epoch:02d} | \"\n        f\"train_loss={train_loss:.4f} | \"\n        f\"val_loss={val_loss:.4f} | \"\n        f\"val_auc={val_auc:.4f}\"\n    )\n\n    if val_auc is not None and val_auc > best_auc:\n        best_auc = val_auc\n        patience_ctr = 0\n        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n    else:\n        patience_ctr += 1\n        if patience_ctr >= PATIENCE:\n            print(\"‚èπ Early stopping triggered\")\n            break\n\n\n# Restore & save best model\nmodel.load_state_dict(best_state)\ntorch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_lstm_crossing.pt\"))\n\n# Save logs\nnp.save(os.path.join(SAVE_DIR, \"train_losses.npy\"), np.array(train_losses))\nnp.save(os.path.join(SAVE_DIR, \"val_losses.npy\"),   np.array(val_losses))\nnp.save(os.path.join(SAVE_DIR, \"val_aucs.npy\"),     np.array(val_aucs))\n\n\n# ======================================================\n# 8) Test Evaluation\n# ======================================================\nmodel.eval()\nys, ps = [], []\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb = xb.to(device)\n        probs = torch.sigmoid(model(xb)).cpu().numpy()\n        ys.append(yb.numpy())\n        ps.append(probs)\n\ny_true = np.vstack(ys).ravel()\ny_prob = np.vstack(ps).ravel()\ny_pred = (y_prob >= 0.5).astype(int)\n\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\"Accuracy:\", accuracy_score(y_true, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_true, y_prob))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\nprint(classification_report(y_true, y_pred, digits=4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FIG_DIR = \"/kaggle/working/phase3_figures\"\nos.makedirs(FIG_DIR, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.plot(epochs, train_losses, label=\"Training Loss\", marker=\"o\")\nplt.plot(epochs, val_losses, label=\"Validation Loss\", marker=\"s\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\nplt.savefig(os.path.join(FIG_DIR, \"loss_curve.png\"), dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.plot(epochs, val_aucs, marker=\"o\", color=\"darkgreen\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"ROC-AUC\")\nplt.title(\"Validation ROC-AUC Over Epochs\")\nplt.ylim(0, 1)\nplt.grid(True)\nplt.tight_layout()\n\nplt.savefig(os.path.join(FIG_DIR, \"val_auc_curve.png\"), dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm,\n    display_labels=[\"Non-crossing\", \"Crossing\"]\n)\n\nplt.figure(figsize=(4,4))\ndisp.plot(cmap=\"Blues\", values_format=\"d\")\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.tight_layout()\n\nplt.savefig(os.path.join(FIG_DIR, \"confusion_matrix_test.png\"), dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nFIG_DIR = \"/kaggle/working/phase3_figures\"\nos.makedirs(FIG_DIR, exist_ok=True)\n\nlabels = [\"Non-crossing (0)\", \"Crossing (1)\"]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion(cm_to_plot, title, fname, fmt=\".2f\", cmap=\"Blues\"):\n    plt.figure(figsize=(5,4))\n    plt.imshow(cm_to_plot, interpolation=\"nearest\", cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    tick_marks = np.arange(len(labels))\n    plt.xticks(tick_marks, labels, rotation=25, ha=\"right\")\n    plt.yticks(tick_marks, labels)\n\n    thresh = cm_to_plot.max() / 2.0\n    for i in range(cm_to_plot.shape[0]):\n        for j in range(cm_to_plot.shape[1]):\n            plt.text(\n                j, i,\n                format(cm_to_plot[i, j], fmt),\n                ha=\"center\", va=\"center\",\n                color=\"white\" if cm_to_plot[i, j] > thresh else \"black\"\n            )\n\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.tight_layout()\n\n    plt.savefig(os.path.join(FIG_DIR, fname), dpi=300)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred)\ncm_norm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\n\nFIG_DIR = \"/kaggle/working/phase3_figures\"\nos.makedirs(FIG_DIR, exist_ok=True)\n\nplt.figure(figsize=(6,4))\n\nplt.hist(\n    y_prob[y_true == 1],\n    bins=15,\n    alpha=0.75,\n    label=\"Crossing (1)\",\n    color=\"tab:blue\",\n    edgecolor=\"black\"\n)\n\nplt.hist(\n    y_prob[y_true == 0],\n    bins=15,\n    alpha=0.75,\n    label=\"Non-crossing (0)\",\n    color=\"tab:orange\",\n    edgecolor=\"black\"\n)\n\nplt.xlabel(\"Predicted Probability of Crossing\")\nplt.ylabel(\"Number of Videos\")\nplt.title(\"Prediction Probability Distribution (Test Set)\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\n\nplt.savefig(\n    os.path.join(FIG_DIR, \"05_probability_distribution.png\"),\n    dpi=300\n)\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nindices = list(range(len(y_true)))\nrandom.shuffle(indices)\n\nprint(\"Random test samples:\\n\")\n\nfor idx in indices[:5]:\n    gt = int(y_true[idx])\n    pred = int(y_pred[idx])\n    prob = float(y_prob[idx])\n    status = \"‚úÖ Correct\" if gt == pred else \"‚ùå Wrong\"\n\n    print(\n        f\"Sample {idx:02d} | \"\n        f\"GT={gt} | Pred={pred} | \"\n        f\"P(crossing)={prob:.3f} | {status}\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_ROOT = \"/kaggle/input/vehic-ped-intuition/images/test\"\nLBL_ROOT = \"/kaggle/input/vehic-ped-intuition/labels/test\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\ndef show_video_sample(video_id, gt, pred, prob):\n    # find frames belonging to this video\n    frames = sorted([\n        f for f in os.listdir(IMG_ROOT)\n        if f.startswith(video_id)\n    ])\n    \n    if len(frames) == 0:\n        print(\"No frames found for\", video_id)\n        return\n\n    # take middle frame (stable & interpretable)\n    fname = frames[len(frames)//2]\n    img_path = os.path.join(IMG_ROOT, fname)\n\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    title = (\n        f\"{video_id}\\n\"\n        f\"GT: {gt} | Pred: {pred} | P(crossing)={prob:.2f}\"\n    )\n\n    plt.figure(figsize=(5,4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(\n    os.path.join(FIG_DIR, f\"{video_id}_gt{gt}_pred{pred}.png\"),\n    dpi=300\n)\nplt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n# Map index ‚Üí video id\ntest_video_ids = np.load(\n    os.path.join(FEAT_DIR, \"test_video_ids.npy\"),\n    allow_pickle=True\n)\n\nindices = list(range(len(y_true)))\nrandom.shuffle(indices)\n\nshown = 0\nfor idx in indices:\n    vid = str(test_video_ids[idx])\n    gt  = int(y_true[idx])\n    pred = int(y_pred[idx])\n    prob = float(y_prob[idx])\n\n    # show both correct and wrong\n    show_video_sample(vid, gt, pred, prob)\n    shown += 1\n    if shown == 5:\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fine-Tuned Training","metadata":{}},{"cell_type":"code","source":"import os, glob, random\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n\n# ======================================================\n# 0) Reproducibility\n# ======================================================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# ======================================================\n# 1) Paths & device\n# ======================================================\nFEAT_DIR = \"/kaggle/working/phase3_video_features\"\nXML_DIR  = \"/kaggle/input/attributes-label/annotations_attributes\"\nSAVE_DIR = \"/kaggle/working/phase3_lstm_finetuned\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\n# ======================================================\n# 2) Load features + video IDs\n# ======================================================\nX_train = np.load(os.path.join(FEAT_DIR, \"X_train_videos.npy\"))\nX_val   = np.load(os.path.join(FEAT_DIR, \"X_val_videos.npy\"))\nX_test  = np.load(os.path.join(FEAT_DIR, \"X_test_videos.npy\"))\n\ntrain_ids = np.load(os.path.join(FEAT_DIR, \"train_video_ids.npy\"), allow_pickle=True)\nval_ids   = np.load(os.path.join(FEAT_DIR, \"val_video_ids.npy\"),   allow_pickle=True)\ntest_ids  = np.load(os.path.join(FEAT_DIR, \"test_video_ids.npy\"),  allow_pickle=True)\n\n# ======================================================\n# 3) XML parsing ‚Üí video-level crossing label\n# ======================================================\ndef parse_crossing(xml_path):\n    try:\n        root = ET.parse(xml_path).getroot()\n    except Exception:\n        return None\n\n    vals = []\n    for ped in root.findall(\".//pedestrian\"):\n        c = ped.attrib.get(\"crossing\", None)\n        if c is not None:\n            try:\n                vals.append(int(c))\n            except:\n                pass\n\n    if not vals:\n        return None\n    if any(v == 1 for v in vals):\n        return 1\n    if any(v == 0 for v in vals):\n        return 0\n    return None\n\nvideo_to_label = {}\nfor xp in glob.glob(os.path.join(XML_DIR, \"video_*_attributes.xml\")):\n    vid = os.path.basename(xp).replace(\"_attributes.xml\", \"\")\n    video_to_label[vid] = parse_crossing(xp)\n\ndef filter_labeled(X, ids):\n    X_out, y_out = [], []\n    for x, vid in zip(X, ids):\n        lab = video_to_label.get(str(vid), None)\n        if lab is None:\n            continue\n        X_out.append(x)\n        y_out.append(lab)\n    return np.asarray(X_out, np.float32), np.asarray(y_out, np.float32)\n\nXtr, ytr = filter_labeled(X_train, train_ids)\nXva, yva = filter_labeled(X_val,   val_ids)\nXte, yte = filter_labeled(X_test,  test_ids)\n\nprint(\"Train:\", Xtr.shape, \"Pos:\", int((ytr==1).sum()), \"Neg:\", int((ytr==0).sum()))\nprint(\"Val:  \", Xva.shape)\nprint(\"Test: \", Xte.shape)\n\n# ======================================================\n# 4) DataLoaders\n# ======================================================\ntrain_loader = DataLoader(\n    TensorDataset(torch.tensor(Xtr), torch.tensor(ytr).view(-1,1)),\n    batch_size=32, shuffle=True\n)\nval_loader = DataLoader(\n    TensorDataset(torch.tensor(Xva), torch.tensor(yva).view(-1,1)),\n    batch_size=32\n)\ntest_loader = DataLoader(\n    TensorDataset(torch.tensor(Xte), torch.tensor(yte).view(-1,1)),\n    batch_size=32\n)\n\n# ======================================================\n# 5) LSTM Classifier (REGULARIZED)\n# ======================================================\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=64, dropout=0.5):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        last = out[:, -1, :]\n        last = self.dropout(last)\n        return self.fc(last)\n\nmodel = LSTMClassifier().to(device)\n\n# ======================================================\n# 6) Loss + Optimizer (imbalance + L2)\n# ======================================================\npos = (ytr == 1).sum()\nneg = (ytr == 0).sum()\npos_weight = torch.tensor([neg / max(pos, 1)], device=device)\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=5e-4,\n    weight_decay=1e-4   # L2 regularization\n)\n\nprint(\"pos_weight:\", float(pos_weight))\n\n# ======================================================\n# 7) Training + Early Stopping (LOSS + AUC)\n# ======================================================\nEPOCHS = 30\nPATIENCE = 6\n\ntrain_losses = []\nval_losses   = []\nval_aucs     = []\n\nbest_auc = -1.0\npatience_ctr = 0\nbest_state = None\n\ndef eval_metrics(loader):\n    model.eval()\n    ys, ps, losses = [], [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            probs = torch.sigmoid(logits)\n            losses.append(loss.item())\n            ys.append(yb.cpu().numpy())\n            ps.append(probs.cpu().numpy())\n\n    y = np.vstack(ys).ravel()\n    p = np.vstack(ps).ravel()\n    auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else None\n    return np.mean(losses), auc\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(xb), yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    train_loss = total_loss / len(train_loader)\n    val_loss, val_auc = eval_metrics(val_loader)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    val_aucs.append(val_auc)\n\n    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_auc={val_auc:.4f}\")\n\n    if val_auc is not None and val_auc > best_auc:\n        best_auc = val_auc\n        patience_ctr = 0\n        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n    else:\n        patience_ctr += 1\n        if patience_ctr >= PATIENCE:\n            print(\"‚èπ Early stopping triggered\")\n            break\n\n# Restore best model\nmodel.load_state_dict(best_state)\ntorch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_finetuned_lstm_crossing.pt\"))\n\nnp.save(os.path.join(SAVE_DIR, \"train_losses.npy\"), np.array(train_losses))\nnp.save(os.path.join(SAVE_DIR, \"val_losses.npy\"),   np.array(val_losses))\nnp.save(os.path.join(SAVE_DIR, \"val_aucs.npy\"),     np.array(val_aucs))\n\n# ======================================================\n# 8) Test Evaluation\n# ======================================================\nmodel.eval()\nys, ps = [], []\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb = xb.to(device)\n        probs = torch.sigmoid(model(xb)).cpu().numpy()\n        ys.append(yb.numpy())\n        ps.append(probs)\n\ny_true = np.vstack(ys).ravel()\ny_prob = np.vstack(ps).ravel()\ny_pred = (y_prob >= 0.5).astype(int)\n\nprint(\"\\n===== TEST RESULTS (FINETUNED) =====\")\nprint(\"Accuracy:\", accuracy_score(y_true, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_true, y_prob))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\nprint(classification_report(y_true, y_pred, digits=4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\n\nFIG_DIR = \"/kaggle/working/figures\"\nos.makedirs(FIG_DIR, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = np.arange(1, len(train_losses) + 1)\n\nplt.figure(figsize=(7,5))\nplt.plot(epochs, train_losses, marker=\"o\", label=\"Training Loss\")\nplt.plot(epochs, val_losses, marker=\"s\", label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss (LSTM Finetuned)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\nplt.savefig(os.path.join(FIG_DIR, \"01_loss_curve.png\"), dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplt.plot(epochs, val_aucs, marker=\"o\", color=\"green\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"ROC-AUC\")\nplt.title(\"Validation ROC-AUC Over Epochs\")\nplt.grid(True)\nplt.tight_layout()\n\nplt.savefig(os.path.join(FIG_DIR, \"02_val_auc_curve.png\"), dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = [\"Non-crossing (0)\", \"Crossing (1)\"]\ncm = confusion_matrix(y_true, y_pred)\ncm_norm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n\ndef plot_cm(cm, title, fname, fmt=\".2f\"):\n    plt.figure(figsize=(5,4))\n    plt.imshow(cm, cmap=\"Blues\")\n    plt.title(title)\n    plt.colorbar()\n    ticks = np.arange(len(labels))\n    plt.xticks(ticks, labels, rotation=25, ha=\"right\")\n    plt.yticks(ticks, labels)\n\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], fmt),\n                     ha=\"center\", va=\"center\")\n\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(FIG_DIR, fname), dpi=300)\n    plt.show()\n\nplot_cm(cm, \"Confusion Matrix (Test Set)\", \"03_confusion_raw.png\", fmt=\"d\")\nplot_cm(cm_norm, \"Confusion Matrix (Normalized)\", \"04_confusion_normalized.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = pd.DataFrame({\n    \"Model\": [\"Baseline LSTM\", \"Finetuned LSTM\"],\n    \"Test Accuracy\": [0.9167, 0.9167],\n    \"Test ROC-AUC\": [0.9802, 0.9683],\n    \"Crossing Recall\": [0.9524, 1.0000],\n    \"Non-crossing Recall\": [0.6667, 0.3333]\n})\n\nresults\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results.to_csv(os.path.join(FIG_DIR, \"06_model_comparison.csv\"), index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,2))\nax.axis(\"off\")\nax.table(cellText=results.values,\n         colLabels=results.columns,\n         loc=\"center\")\nplt.tight_layout()\nplt.savefig(os.path.join(FIG_DIR, \"06_model_comparison_table.png\"), dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Full Pipeline for preparing sequence folders before preprocessing on ViT","metadata":{}},{"cell_type":"code","source":"import os, glob, json, zipfile, shutil\nimport cv2\nimport numpy as np\nimport xml.etree.ElementTree as ET\nfrom collections import Counter\nfrom ultralytics import YOLO\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset dirs\nIMG_ROOT = \"/kaggle/input/vehic-ped-intuition/images\"\nXML_GLOB = \"/kaggle/input/attributes-label/annotations_attributes/video_*_*.xml\"\n\nSPLITS = [\"train\", \"val\", \"test\"]\n\n# YOLO+Tracking\nMODEL_PATH = \"/kaggle/input/first-phase-model/weights/best.pt\"\nCONF = 0.33\nIMGSZ = 640\nTRACKER = \"botsort.yaml\"\n\n# Filters (freeze these)\nMIN_TRACK_LEN = 16\nINTENTION_CONF = 0.50\n\n# Relevance filtering (distance proxy)\nMIN_MEDIAN_HEIGHT = 90     # pixels\nMIN_HEIGHT_GROWTH = 15     # pixels\n\n# Duplicate suppression\nDUP_IOU_THR = 0.70\n\n# Cropping (context-aware)\nEXPAND_RATIO = 1.8\n\n# Sequences\nSEQ_LEN = 16\nSTRIDE = 4\n\n# Decision safety\nUSE_DECISION_POINT_IF_AVAILABLE = True\nFALLBACK_CUTOFF_RATIO = 0.8  # used when decision_point == -1 or not present\n\n# Output\nOUT_ROOT = \"/kaggle/working/intent_sequences_dataset\"\nos.makedirs(OUT_ROOT, exist_ok=True)\n\n# If True: only process videos where XML contains exactly 1 pedestrian\n# (Recommended if you do not have a reliable mapping between XML ped IDs and tracker IDs)\nREQUIRE_SINGLE_PED_XML = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Helper Functions (tracking + filter +XML + Cropping + Sequences)\n* IoU, relevance, confidence, duplicate suppression\n* XML parsing (attributes only)\n* Crop with context\n* Temporal cutoff using decision_point (if available)\n* Build XML index (fast lookup)\n* Initialize YOLO once\n* Main Batch Runner (ALL videos in a split)\n* Run ALL splits and save dataset (crops as sequences)","metadata":{}},{"cell_type":"code","source":"def iou(a, b):\n    x1, y1 = max(a[0], b[0]), max(a[1], b[1])\n    x2, y2 = min(a[2], b[2]), min(a[3], b[3])\n    inter = max(0, x2 - x1) * max(0, y2 - y1)\n    areaA = max(0, (a[2] - a[0])) * max(0, (a[3] - a[1]))\n    areaB = max(0, (b[2] - b[0])) * max(0, (b[3] - b[1]))\n    return inter / (areaA + areaB - inter + 1e-6)\n\ndef bbox_heights(seq):\n    return np.array([(b[3] - b[1]) for (_, b, _) in seq], dtype=np.float32)\n\ndef is_relevant(seq):\n    h = bbox_heights(seq)\n    return (np.median(h) >= MIN_MEDIAN_HEIGHT) or ((h[-1] - h[0]) >= MIN_HEIGHT_GROWTH)\n\ndef high_conf(seq):\n    return float(np.mean([c for (_, _, c) in seq])) >= INTENTION_CONF\n\ndef mean_iou_tracks(seq1, seq2):\n    n = min(len(seq1), len(seq2))\n    if n < 5:\n        return 0.0\n    return float(np.mean([iou(seq1[i][1], seq2[i][1]) for i in range(n)]))\n\ndef suppress_duplicates(tracks):\n    # keep longer tracks first\n    tids = sorted(tracks.keys(), key=lambda t: len(tracks[t]), reverse=True)\n    kept = {}\n    for tid in tids:\n        dup = False\n        for kt in kept.keys():\n            if mean_iou_tracks(tracks[tid], kept[kt]) > DUP_IOU_THR:\n                dup = True\n                break\n        if not dup:\n            kept[tid] = tracks[tid]\n    return kept\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_pedestrian_attributes(xml_path):\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n\n    peds = []\n    for ped in root.findall(\".//pedestrian\"):\n        d = dict(ped.attrib)\n        # cast fields if present\n        if \"crossing\" in d:\n            try: d[\"crossing\"] = int(d[\"crossing\"])\n            except: pass\n        if \"decision_point\" in d:\n            try: d[\"decision_point\"] = int(d[\"decision_point\"])\n            except: d[\"decision_point\"] = -1\n        if \"crossing_point\" in d:\n            try: d[\"crossing_point\"] = int(d[\"crossing_point\"])\n            except: d[\"crossing_point\"] = -1\n        peds.append(d)\n\n    return peds\n\ndef label_from_ped_attr(ped_attr):\n    # Your dataset: crossing = -1 means NOT crossing; crossing = 1 means crossing\n    c = ped_attr.get(\"crossing\", None)\n    if c == 1:\n        return \"crossing\"\n    return \"not_crossing\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_with_context(frame_path, box, expand_ratio=1.8):\n    img = cv2.imread(frame_path)\n    if img is None:\n        return None\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    H, W = img.shape[:2]\n\n    x1, y1, x2, y2 = map(float, box)\n    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n    bw, bh = (x2 - x1) * expand_ratio, (y2 - y1) * expand_ratio\n\n    nx1 = int(max(0, cx - bw / 2))\n    ny1 = int(max(0, cy - bh / 2))\n    nx2 = int(min(W, cx + bw / 2))\n    ny2 = int(min(H, cy + bh / 2))\n\n    if nx2 <= nx1 or ny2 <= ny1:\n        return None\n\n    crop = img[ny1:ny2, nx1:nx2]\n    if crop.size == 0:\n        return None\n    return crop\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_windows(track_seq, seq_len=16, stride=4):\n    windows = []\n    for i in range(0, len(track_seq) - seq_len + 1, stride):\n        windows.append(track_seq[i:i + seq_len])\n    return windows\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_temporal_cutoff(track_seq, ped_attr):\n    if not USE_DECISION_POINT_IF_AVAILABLE:\n        cut = int(len(track_seq) * FALLBACK_CUTOFF_RATIO)\n        return track_seq[:max(cut, SEQ_LEN)]\n\n    dp = ped_attr.get(\"decision_point\", -1)\n    if isinstance(dp, int) and dp >= 0:\n        # keep only frames <= decision point\n        seq = [x for x in track_seq if x[0] <= dp]\n        if len(seq) >= SEQ_LEN:\n            return seq\n\n    # fallback\n    cut = int(len(track_seq) * FALLBACK_CUTOFF_RATIO)\n    return track_seq[:max(cut, SEQ_LEN)]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xml_files = sorted(glob.glob(XML_GLOB))\nprint(\"XML files:\", len(xml_files))\n\n# Store by filename (basename without extension)\nxml_by_base = {os.path.basename(x).replace(\".xml\", \"\"): x for x in xml_files}\nall_xml_paths = set(xml_files)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_xml_for_video(video_id):\n    # best effort: any xml basename containing video_id\n    candidates = [p for p in xml_files if video_id in os.path.basename(p)]\n    if len(candidates) == 1:\n        return candidates[0]\n    if len(candidates) > 1:\n        # pick shortest basename match (often most specific)\n        candidates = sorted(candidates, key=lambda p: len(os.path.basename(p)))\n        return candidates[0]\n    return None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo = YOLO(MODEL_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_video_ids(split):\n    img_dir = f\"{IMG_ROOT}/{split}\"\n    vids = sorted(set(f.split(\"_f\")[0] for f in os.listdir(img_dir)))\n    return vids\n\ndef list_frames(split, video_id):\n    img_dir = f\"{IMG_ROOT}/{split}\"\n    frs = sorted([\n        os.path.join(img_dir, f)\n        for f in os.listdir(img_dir)\n        if f.startswith(video_id)\n    ])\n    return frs\n\ndef run_tracking(frames):\n    track_db = {}\n    for fidx, frame_path in enumerate(frames):\n        img = cv2.imread(frame_path)\n        if img is None:\n            continue\n        r = yolo.track(img, conf=CONF, imgsz=IMGSZ, persist=True, tracker=TRACKER, verbose=False)[0]\n        if r.boxes is None or r.boxes.id is None:\n            continue\n        boxes = r.boxes.xyxy.cpu().numpy()\n        ids   = r.boxes.id.cpu().numpy().astype(int)\n        confs = r.boxes.conf.cpu().numpy()\n        for box, tid, c in zip(boxes, ids, confs):\n            if tid == -1:\n                continue\n            track_db.setdefault(int(tid), []).append((fidx, box, float(c)))\n    return track_db\n\ndef filter_tracks(track_db):\n    # length\n    t = {tid: seq for tid, seq in track_db.items() if len(seq) >= MIN_TRACK_LEN}\n    # relevance\n    t = {tid: seq for tid, seq in t.items() if is_relevant(seq)}\n    # confidence\n    t = {tid: seq for tid, seq in t.items() if high_conf(seq)}\n    # duplicates\n    t = suppress_duplicates(t)\n    return t\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequence_index = []   # global index rows (we save to CSV later)\nfail_log = []         # list of dicts describing failures\n\nfor split in SPLITS:\n    print(\"\\n====================\")\n    print(\"PROCESSING SPLIT:\", split)\n    print(\"====================\")\n\n    split_out = os.path.join(OUT_ROOT, split)\n    os.makedirs(split_out, exist_ok=True)\n\n    video_ids = list_video_ids(split)\n    print(\"Videos found:\", len(video_ids))\n\n    for vid_i, video_id in enumerate(video_ids):\n        frames = list_frames(split, video_id)\n        if len(frames) < SEQ_LEN:\n            fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"too_few_frames\"})\n            continue\n\n        # 1) find XML\n        xml_path = find_xml_for_video(video_id)\n        if xml_path is None:\n            fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"xml_not_found\"})\n            continue\n\n        # 2) parse pedestrians in XML\n        peds = parse_pedestrian_attributes(xml_path)\n        if len(peds) == 0:\n            fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"xml_no_pedestrians\"})\n            continue\n\n        if REQUIRE_SINGLE_PED_XML and len(peds) != 1:\n            fail_log.append({\"split\": split, \"video\": video_id, \"reason\": f\"xml_ped_count_{len(peds)}\"})\n            continue\n\n        # 3) tracking\n        track_db = run_tracking(frames)\n        if len(track_db) == 0:\n            fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"no_tracks\"})\n            continue\n\n        # 4) filtering\n        filtered = filter_tracks(track_db)\n        if len(filtered) == 0:\n            fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"no_tracks_after_filter\"})\n            continue\n\n        # 5) pick track(s) and label\n        if len(peds) == 1:\n            ped = peds[0]\n            label = label_from_ped_attr(ped)\n\n            # dominant track = longest filtered track\n            selected_tid = max(filtered.keys(), key=lambda t: len(filtered[t]))\n            track = filtered[selected_tid]\n\n            # decision cutoff\n            track = apply_temporal_cutoff(track, ped)\n            if len(track) < SEQ_LEN:\n                fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"track_too_short_after_cutoff\"})\n                continue\n\n            # 6) make sequences\n            windows = build_windows(track, SEQ_LEN, STRIDE)\n            if len(windows) == 0:\n                fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"no_windows\"})\n                continue\n\n            # 7) save sequences\n            video_out = os.path.join(split_out, video_id)\n            seq_out_root = os.path.join(video_out, \"sequences\")\n            os.makedirs(seq_out_root, exist_ok=True)\n\n            seq_counter = 0\n            saved_any = False\n\n            for w in windows:\n                # crop all frames in window\n                crops = []\n                for fidx, box, conf in w:\n                    crop = crop_with_context(frames[fidx], box, EXPAND_RATIO)\n                    if crop is None:\n                        crops = []\n                        break\n                    crops.append((fidx, crop))\n\n                if len(crops) != SEQ_LEN:\n                    continue\n\n                seq_id = f\"{video_id}_tid{selected_tid}_seq{seq_counter:05d}\"\n                seq_dir = os.path.join(seq_out_root, seq_id)\n                os.makedirs(seq_dir, exist_ok=True)\n\n                for j, (fidx, crop) in enumerate(crops):\n                    out_path = os.path.join(seq_dir, f\"{j:02d}_frame_{fidx:05d}.jpg\")\n                    cv2.imwrite(out_path, cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))\n\n                sequence_index.append({\n                    \"split\": split,\n                    \"video_id\": video_id,\n                    \"xml\": os.path.basename(xml_path),\n                    \"tid\": int(selected_tid),\n                    \"seq_id\": seq_id,\n                    \"label\": label,\n                    \"crossing\": int(ped.get(\"crossing\", -1)) if isinstance(ped.get(\"crossing\", -1), int) else -1,\n                    \"decision_point\": int(ped.get(\"decision_point\", -1)) if isinstance(ped.get(\"decision_point\", -1), int) else -1,\n                    \"start_frame\": int(crops[0][0]),\n                    \"end_frame\": int(crops[-1][0]),\n                    \"seq_dir\": seq_dir,\n                })\n\n                seq_counter += 1\n                saved_any = True\n\n            if not saved_any:\n                fail_log.append({\"split\": split, \"video\": video_id, \"reason\": \"all_windows_failed_cropping\"})\n                continue\n\n        # progress print sometimes\n        if (vid_i + 1) % 50 == 0:\n            print(f\"[{split}] Processed {vid_i+1}/{len(video_ids)} videos. Seqs so far: {len(sequence_index)}\")\n\nprint(\"\\nDONE.\")\nprint(\"Total sequences saved:\", len(sequence_index))\nprint(\"Failures:\", len(fail_log))\nprint(\"Label distribution:\", Counter([r[\"label\"] for r in sequence_index]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\n\nindex_csv = os.path.join(OUT_ROOT, \"sequence_index.csv\")\nwith open(index_csv, \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=list(sequence_index[0].keys()) if sequence_index else [])\n    if sequence_index:\n        writer.writeheader()\n        writer.writerows(sequence_index)\n\nfail_json = os.path.join(OUT_ROOT, \"fail_log.json\")\nwith open(fail_json, \"w\") as f:\n    json.dump(fail_log, f, indent=2)\n\nprint(\"Saved index:\", index_csv)\nprint(\"Saved fail log:\", fail_json)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = os.path.join(\"/kaggle/working\", \"intent_sequences_dataset.zip\")\n\ndef zipdir(folder, ziph):\n    for root, dirs, files in os.walk(folder):\n        for file in files:\n            full_path = os.path.join(root, file)\n            rel_path = os.path.relpath(full_path, folder)\n            ziph.write(full_path, arcname=rel_path)\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    # include index and fail log\n    if os.path.exists(index_csv):\n        z.write(index_csv, arcname=\"sequence_index.csv\")\n    if os.path.exists(fail_json):\n        z.write(fail_json, arcname=\"fail_log.json\")\n    # include all split folders\n    zipdir(OUT_ROOT, z)\n\nprint(\"Zipped dataset to:\", zip_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seqs_per_video = idx.groupby(\"video_id\").size()\n\nprint(\"Min sequences per video:\", seqs_per_video.min())\nprint(\"Max sequences per video:\", seqs_per_video.max())\nprint(seqs_per_video.describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ViT Preprocessing","metadata":{}},{"cell_type":"code","source":"import os, glob, csv\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport timm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vit_preprocess = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nvit = timm.create_model(\n    \"vit_base_patch16_224\",\n    pretrained=True,\n    num_classes=0  # feature extractor\n)\nvit.eval()\nvit.to(device)\n\nprint(\"ViT feature dim:\", vit.num_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_sequence_tensor(row, T_expected=16):\n    seq_dir = resolve_seq_dir(row)\n    frames = sorted([f for f in os.listdir(seq_dir) if f.endswith(\".jpg\")])\n\n    assert len(frames) == T_expected, \"‚ùå Sequence length mismatch\"\n\n    imgs = []\n    for f in frames:\n        img = cv2.cvtColor(\n            cv2.imread(os.path.join(seq_dir, f)),\n            cv2.COLOR_BGR2RGB\n        )\n        imgs.append(vit_preprocess(img))\n\n    return torch.stack(imgs, dim=0)  # [T,3,224,224]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = idx.sample(1, random_state=1).iloc[0]\n\nprint(\"Testing seq:\", row[\"seq_id\"], \"| label:\", row[\"label\"])\n\nx = load_sequence_tensor(row)\nprint(\"Input tensor shape:\", x.shape)\n\nwith torch.no_grad():\n    feats = vit(x.to(device))\n\nprint(\"Output feature shape:\", feats.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nFEAT_ROOT = \"/kaggle/working/vit_features\"\nos.makedirs(FEAT_ROOT, exist_ok=True)\n\nprint(\"Feature output dir:\", FEAT_ROOT)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_sequence_tensor(row, T_expected=16):\n    seq_dir = resolve_seq_dir(row)\n    frames = sorted([f for f in os.listdir(seq_dir) if f.endswith(\".jpg\")])\n    if len(frames) != T_expected:\n        return None\n\n    imgs = []\n    for f in frames:\n        img = cv2.cvtColor(\n            cv2.imread(os.path.join(seq_dir, f)),\n            cv2.COLOR_BGR2RGB\n        )\n        imgs.append(vit_preprocess(img))\n\n    return torch.stack(imgs, dim=0)  # [T,3,224,224]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\n\nfeatures_index = []\n\nvit.eval()\ntorch.set_grad_enabled(False)\n\nfor split in [\"train\", \"val\", \"test\"]:\n    split_df = idx[idx[\"split\"] == split]\n    out_dir = os.path.join(FEAT_ROOT, split)\n    os.makedirs(out_dir, exist_ok=True)\n\n    print(f\"\\nProcessing split: {split} | sequences: {len(split_df)}\")\n\n    for _, row in tqdm(split_df.iterrows(), total=len(split_df)):\n        seq_id = row[\"seq_id\"]\n        out_path = os.path.join(out_dir, f\"{seq_id}.pt\")\n\n        # Skip if already computed (safe resume)\n        if os.path.exists(out_path):\n            continue\n\n        x = load_sequence_tensor(row)\n        if x is None:\n            continue\n\n        x = x.to(device)\n\n        feats = vit(x).cpu()  # [T,768]\n\n        torch.save(\n            {\n                \"seq_id\": seq_id,\n                \"features\": feats,\n                \"label\": row[\"label\"],\n                \"split\": split,\n                \"video_id\": row[\"video_id\"],\n                \"tid\": int(row[\"tid\"]),\n            },\n            out_path\n        )\n\n        features_index.append(\n            {\n                \"seq_id\": seq_id,\n                \"split\": split,\n                \"label\": row[\"label\"],\n                \"pt_path\": out_path,\n            }\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feat_index_path = os.path.join(FEAT_ROOT, \"features_index.csv\")\npd.DataFrame(features_index).to_csv(feat_index_path, index=False)\n\nprint(\"Saved feature index:\", feat_index_path)\nprint(\"Total feature tensors:\", len(features_index))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = pd.read_csv(feat_index_path).sample(3, random_state=0)\n\nfor _, r in sample.iterrows():\n    d = torch.load(r[\"pt_path\"])\n    print(\n        r[\"seq_id\"],\n        d[\"features\"].shape,\n        d[\"label\"]\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM Phase","metadata":{}},{"cell_type":"code","source":"import os, copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nimport cv2\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURE_ROOT = \"/kaggle/input/vit-features\"\nFEATURE_INDEX = os.path.join(FEATURE_ROOT, \"features_index.csv\")\n\nSEQ_ROOT = \"/kaggle/input/dataset-sequences/intent_sequences_dataset\"\nSEQ_INDEX = os.path.join(SEQ_ROOT, \"sequence_index_final.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Loader","metadata":{}},{"cell_type":"code","source":"def resolve_pt_path(row):\n    return os.path.join(\n        FEATURE_ROOT,\n        row[\"split\"],\n        f\"{row['seq_id']}.pt\"\n    )\n\nclass IntentDataset(Dataset):\n    def __init__(self, index_csv, split):\n        df = pd.read_csv(index_csv)\n        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        data = torch.load(resolve_pt_path(row))\n        x = data[\"features\"]            # [16, 768]\n        y = 1 if row[\"label\"] == \"crossing\" else 0\n        return x, y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = IntentDataset(FEATURE_INDEX, \"train\")\nval_ds   = IntentDataset(FEATURE_INDEX, \"val\")\ntest_ds  = IntentDataset(FEATURE_INDEX, \"test\")\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=32)\ntest_loader  = DataLoader(test_ds, batch_size=32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"class IntentLSTM(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=128, dropout=0.3):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim, 2)\n\n    def forward(self, x):\n        _, (h, _) = self.lstm(x)\n        h = self.dropout(h[-1])\n        return self.fc(h)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    total_loss, correct, total = 0, 0, 0\n\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n\n        if train:\n            optimizer.zero_grad()\n\n        out = model(x)\n        loss = criterion(out, y)\n\n        if train:\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        total_loss += loss.item() * x.size(0)\n        correct += (out.argmax(1) == y).sum().item()\n        total += y.size(0)\n\n    return total_loss / total, correct / total\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = IntentLSTM().to(device)\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=1e-3,\n    weight_decay=1e-4\n)\n\ncriterion = nn.CrossEntropyLoss()\n\nbest_state = None\nbest_val_loss = float(\"inf\")\npatience = 5\npat_left = patience\n\nEPOCHS = 30\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = run_epoch(model, train_loader, optimizer)\n    val_loss, val_acc     = run_epoch(model, val_loader)\n\n    print(f\"Epoch {epoch:02d} | \"\n          f\"Train loss {train_loss:.3f} acc {train_acc:.3f} | \"\n          f\"Val loss {val_loss:.3f} acc {val_acc:.3f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_state = copy.deepcopy(model.state_dict())\n        pat_left = patience\n    else:\n        pat_left -= 1\n        if pat_left == 0:\n            print(\"Early stopping\")\n            break\n\nmodel.load_state_dict(best_state)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"evaluation","metadata":{}},{"cell_type":"code","source":"model.eval()\nys, preds = [], []\n\nwith torch.no_grad():\n    for x, y in test_loader:\n        x = x.to(device)\n        out = model(x)\n        preds.extend(out.argmax(1).cpu().tolist())\n        ys.extend(y.tolist())\n\nprint(\"Accuracy:\", accuracy_score(ys, preds))\nprint(\"F1:\", f1_score(ys, preds))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(ys, preds))\nprint(classification_report(ys, preds, target_names=[\"not_crossing\", \"crossing\"]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Predictions with Frames","metadata":{}},{"cell_type":"code","source":"feat_idx = pd.read_csv(FEATURE_INDEX)\nseq_idx  = pd.read_csv(SEQ_INDEX)\n\n# Merge to get video_id for each feature row\nmerged = feat_idx.merge(\n    seq_idx[[\"seq_id\", \"video_id\", \"split\"]],\n    on=[\"seq_id\", \"split\"],\n    how=\"left\"\n)\n\n# Keep only test split\ntest_merged = merged[merged[\"split\"] == \"test\"].reset_index(drop=True)\n\nprint(\"Total test sequences:\", len(test_merged))\nprint(\"Unique videos:\", test_merged[\"video_id\"].nunique())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One random sequence per video\nsamples = (\n    test_merged\n    .groupby(\"video_id\", group_keys=False)\n    .apply(lambda x: x.sample(1))\n    .sample(10, random_state=42)\n    .reset_index(drop=True)\n)\n\nprint(\"Sampled sequences from videos:\")\nprint(samples[[\"video_id\", \"seq_id\", \"label\"]])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_sequence(seq_dir, title):\n    frames = sorted(os.listdir(seq_dir))\n    picks = np.linspace(0, len(frames)-1, 8).astype(int)\n\n    plt.figure(figsize=(16,3))\n    for i, k in enumerate(picks):\n        img = cv2.imread(os.path.join(seq_dir, frames[k]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(1, len(picks), i+1)\n        plt.imshow(img)\n        plt.axis(\"off\")\n\n    plt.suptitle(title, fontsize=13)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nfor _, row in samples.iterrows():\n    # Load features\n    data = torch.load(resolve_pt_path(row))\n    x = data[\"features\"].unsqueeze(0).to(device)\n\n    # Predict\n    with torch.no_grad():\n        logits = model(x)\n        pred = logits.argmax(1).item()\n\n    pred_label = \"crossing\" if pred == 1 else \"not_crossing\"\n    true_label = row[\"label\"]\n\n    print(\"=\" * 70)\n    print(f\"VIDEO  : {row['video_id']}\")\n    print(f\"SEQ_ID : {row['seq_id']}\")\n    print(f\"TRUE   : {true_label}\")\n    print(f\"PRED   : {pred_label}\")\n\n    seq_dir = os.path.join(\n        SEQ_ROOT,\n        row[\"split\"],\n        row[\"video_id\"],\n        \"sequences\",\n        row[\"seq_id\"]\n    )\n\n    visualize_sequence(\n        seq_dir,\n        title=f\"VIDEO {row['video_id']} | TRUE={true_label} | PRED={pred_label}\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nSEQ_INDEX = \"/kaggle/input/dataset-sequences/intent_sequences_dataset/sequence_index_final.csv\"\n\nseq_idx = pd.read_csv(SEQ_INDEX)\nprint(seq_idx.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_videos = set(seq_idx[\"video_id\"].unique())\n\nprint(\"Used videos:\", len(all_videos))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tuned parameters","metadata":{}},{"cell_type":"code","source":"import os, glob, json, shutil, random\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt\n\nfrom ultralytics import YOLO\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\nIMG_ROOT = \"/kaggle/input/vehic-ped-intuition/images\"\nXML_GLOB = \"/kaggle/input/attributes-label/annotations_attributes/video_*_*.xml\"\nSPLITS = [\"train\", \"val\", \"test\"]\n\n# YOLO + tracking\nMODEL_PATH = \"/kaggle/input/first-phase-model/weights/best.pt\"\nCONF = 0.33\nIMGSZ = 640\nTRACKER = \"botsort.yaml\"\n\n# Minimal keep (dataset construction)\nMIN_TRACK_LEN = 8\n\n# Stability filter (anti-drift)\nMIN_STABILITY_IOU = 0.25\n\n# Duplicate suppression\nDUP_IOU_THR = 0.70\n\n# Cropping context\nEXPAND_RATIO = 1.8\n\n# Sequences\nSEQ_LEN = 16\nSTRIDE = 4\n\n# Temporal cutoff\nUSE_DECISION_POINT_IF_AVAILABLE = True\nFALLBACK_CUTOFF_RATIO = 0.8\n\n# Output\nOUT_ROOT = \"/kaggle/working/intent_sequences_dataset_clean\"\nos.makedirs(OUT_ROOT, exist_ok=True)\n\nprint(\"OUT_ROOT:\", OUT_ROOT)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo = YOLO(MODEL_PATH)\nprint(\"Loaded YOLO:\", MODEL_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou(a, b):\n    x1, y1 = max(a[0], b[0]), max(a[1], b[1])\n    x2, y2 = min(a[2], b[2]), min(a[3], b[3])\n    inter = max(0, x2 - x1) * max(0, y2 - y1)\n    areaA = max(0, (a[2] - a[0])) * max(0, (a[3] - a[1]))\n    areaB = max(0, (b[2] - b[0])) * max(0, (b[3] - b[1]))\n    return inter / (areaA + areaB - inter + 1e-6)\n\ndef track_stability(seq):\n    if len(seq) < 2:\n        return 0.0\n    ious = [iou(seq[i-1][1], seq[i][1]) for i in range(1, len(seq))]\n    return float(np.mean(ious))\n\ndef bbox_heights(seq):\n    return np.array([(b[3] - b[1]) for (_, b, _) in seq], dtype=np.float32)\n\ndef track_metrics(seq):\n    h = bbox_heights(seq)\n    avg_conf = float(np.mean([c for (_, _, c) in seq])) if len(seq) else 0.0\n    med_h = float(np.median(h)) if len(h) else 0.0\n    growth = float(h[-1] - h[0]) if len(h) else 0.0\n    return {\n        \"len\": int(len(seq)),\n        \"avg_conf\": avg_conf,\n        \"median_h\": med_h,\n        \"growth_h\": growth,\n        \"stability\": track_stability(seq),\n    }\n\ndef mean_iou_tracks(seq1, seq2):\n    n = min(len(seq1), len(seq2))\n    if n < 5:\n        return 0.0\n    return float(np.mean([iou(seq1[i][1], seq2[i][1]) for i in range(n)]))\n\ndef suppress_duplicates(tracks):\n    tids = sorted(tracks.keys(), key=lambda t: len(tracks[t]), reverse=True)\n    kept = {}\n    for tid in tids:\n        dup = False\n        for kt in kept.keys():\n            if mean_iou_tracks(tracks[tid], kept[kt]) > DUP_IOU_THR:\n                dup = True\n                break\n        if not dup:\n            kept[tid] = tracks[tid]\n    return kept\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_pedestrian_attributes(xml_path):\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    peds = []\n    for ped in root.findall(\".//pedestrian\"):\n        d = dict(ped.attrib)\n        for k in [\"crossing\", \"decision_point\", \"crossing_point\"]:\n            if k in d:\n                try:\n                    d[k] = int(d[k])\n                except:\n                    d[k] = -999\n        peds.append(d)\n    return peds\n\ndef label_from_ped_attr(ped):\n    if ped.get(\"crossing\") == 1:\n        return \"crossing\"\n    if ped.get(\"crossing\") in [-1, 0]:\n        return \"not_crossing\"\n    return \"unknown\"\n\ndef find_xml_for_video(video_id):\n    cands = [x for x in xml_files if video_id in os.path.basename(x)]\n    if len(cands) == 0:\n        return None\n    return sorted(cands, key=lambda x: len(os.path.basename(x)))[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xml_files = sorted(glob.glob(XML_GLOB))\nprint(\"XML files:\", len(xml_files))\n\ndef find_xml_for_video(video_id):\n    candidates = [p for p in xml_files if video_id in os.path.basename(p)]\n    if len(candidates) == 1:\n        return candidates[0]\n    if len(candidates) > 1:\n        candidates = sorted(candidates, key=lambda p: len(os.path.basename(p)))\n        return candidates[0]\n    return None\n\ndef parse_pedestrian_attributes(xml_path):\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n    except Exception:\n        return []\n\n    peds = []\n    for ped in root.findall(\".//pedestrian\"):\n        d = dict(ped.attrib)\n        for k in [\"crossing\", \"decision_point\", \"crossing_point\"]:\n            if k in d:\n                try:\n                    d[k] = int(d[k])\n                except:\n                    d[k] = -1\n        peds.append(d)\n    return peds\n\ndef label_from_ped_attr(ped_attr):\n    c = ped_attr.get(\"crossing\", None)\n    if c == 1:\n        return \"crossing\"\n    if c in [0, -1]:\n        return \"not_crossing\"\n    return \"unknown\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_video_ids(split):\n    img_dir = f\"{IMG_ROOT}/{split}\"\n    return sorted(set(f.split(\"_f\")[0] for f in os.listdir(img_dir)))\n\ndef list_frames(split, video_id):\n    img_dir = f\"{IMG_ROOT}/{split}\"\n    frames = sorted([\n        os.path.join(img_dir, f)\n        for f in os.listdir(img_dir)\n        if f.startswith(video_id)\n    ])\n    return frames\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_tracking(frames):\n    track_db = {}\n    for fidx, frame_path in enumerate(frames):\n        img = cv2.imread(frame_path)\n        if img is None:\n            continue\n\n        r = yolo.track(\n            img,\n            conf=CONF,\n            imgsz=IMGSZ,\n            persist=True,\n            tracker=TRACKER,\n            verbose=False\n        )[0]\n\n        if r.boxes is None or r.boxes.id is None:\n            continue\n\n        boxes = r.boxes.xyxy.cpu().numpy()\n        ids   = r.boxes.id.cpu().numpy().astype(int)\n        confs = r.boxes.conf.cpu().numpy()\n\n        for box, tid, c in zip(boxes, ids, confs):\n            if tid == -1:\n                continue\n            track_db.setdefault(int(tid), []).append((fidx, box, float(c)))\n\n    return track_db\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_tracks(track_db):\n    # length\n    t = {tid: seq for tid, seq in track_db.items() if len(seq) >= MIN_TRACK_LEN}\n    # stability\n    t = {tid: seq for tid, seq in t.items() if track_stability(seq) >= MIN_STABILITY_IOU}\n    # duplicates\n    t = suppress_duplicates(t)\n    return t\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def select_best_track(tracks):\n    best_tid, best_score = None, -1e9\n    for tid, seq in tracks.items():\n        m = track_metrics(seq)\n        score = (\n            1.0 * m[\"len\"] +\n            0.03 * m[\"median_h\"] +\n            2.0 * m[\"avg_conf\"] +\n            0.02 * m[\"growth_h\"] +\n            5.0 * m[\"stability\"]\n        )\n        if score > best_score:\n            best_score = score\n            best_tid = tid\n    return best_tid, best_score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_tracks(img_bgr, boxes, ids, color=(0,255,255)):\n    for box, tid in zip(boxes, ids):\n        x1,y1,x2,y2 = map(int, box)\n        cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 2)\n        cv2.putText(img_bgr, f\"ID {tid}\", (x1, max(0,y1-5)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\ndef save_tracking_video(frames, track_db, out_path, fps=10, only_tids=None, title=None):\n    first = cv2.imread(frames[0])\n    H, W = first.shape[:2]\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    vw = cv2.VideoWriter(out_path, fourcc, fps, (W, H))\n\n    # Build quick lookup: frame -> list of (box, tid)\n    by_frame = {}\n    for tid, seq in track_db.items():\n        if only_tids is not None and tid not in only_tids:\n            continue\n        for fidx, box, _ in seq:\n            by_frame.setdefault(fidx, []).append((box, tid))\n\n    for fidx, fp in enumerate(frames):\n        img = cv2.imread(fp)\n        if img is None:\n            continue\n        items = by_frame.get(fidx, [])\n        if items:\n            boxes = [b for (b, _) in items]\n            ids   = [t for (_, t) in items]\n            draw_tracks(img, boxes, ids)\n        if title:\n            cv2.putText(img, title, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2)\n        vw.write(img)\n\n    vw.release()\n    print(\"Saved video:\", out_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_temporal_cutoff(track_seq, ped_attr):\n    if not USE_DECISION_POINT_IF_AVAILABLE:\n        cut = int(len(track_seq) * FALLBACK_CUTOFF_RATIO)\n        return track_seq[:max(cut, 1)]\n\n    dp = ped_attr.get(\"decision_point\", -1)\n    if isinstance(dp, int) and dp >= 0:\n        seq = [x for x in track_seq if x[0] <= dp]\n        if len(seq) > 0:\n            return seq\n\n    cut = int(len(track_seq) * FALLBACK_CUTOFF_RATIO)\n    return track_seq[:max(cut, 1)]\n\ndef build_windows(track_seq, seq_len=16, stride=4):\n    if len(track_seq) < seq_len:\n        return []\n    return [track_seq[i:i+seq_len] for i in range(0, len(track_seq)-seq_len+1, stride)]\n\ndef crop_with_context(frame_path, box, expand_ratio=1.8):\n    img = cv2.imread(frame_path)\n    if img is None:\n        return None\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    H, W = img.shape[:2]\n\n    x1, y1, x2, y2 = map(float, box)\n    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n    bw, bh = (x2 - x1) * expand_ratio, (y2 - y1) * expand_ratio\n\n    nx1 = int(max(0, cx - bw / 2))\n    ny1 = int(max(0, cy - bh / 2))\n    nx2 = int(min(W, cx + bw / 2))\n    ny2 = int(min(H, cy + bh / 2))\n\n    if nx2 <= nx1 or ny2 <= ny1:\n        return None\n    crop = img[ny1:ny2, nx1:nx2]\n    return crop if crop.size else None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split = \"train\"\nvideo_id = random.choice(list_video_ids(split))\nframes = list_frames(split, video_id)\n\nprint(\"Smoking video:\", video_id, \"frames:\", len(frames))\n\n# XML\nxml_path = find_xml_for_video(video_id)\npeds = parse_pedestrian_attributes(xml_path) if xml_path else []\nped = peds[0] if len(peds)==1 else {\"decision_point\":-1, \"crossing\":-1}\nlabel = label_from_ped_attr(ped) if len(peds)==1 else \"unknown\"\n\n# Tracking raw\ntrack_raw = run_tracking(frames)\n\n# Save raw tracking video\nraw_path = f\"{OUT_ROOT}/{video_id}_tracking_raw.mp4\"\nsave_tracking_video(frames, track_raw, raw_path, fps=10, title=\"RAW TRACKS\")\n\n# Filter tracks\ntrack_f = filter_tracks(track_raw)\n\nfiltered_path = f\"{OUT_ROOT}/{video_id}_tracking_filtered.mp4\"\nsave_tracking_video(frames, track_f, filtered_path, fps=10, title=\"FILTERED TRACKS\")\n\nprint(\"Raw tracks:\", len(track_raw), \"Filtered tracks:\", len(track_f), \"Label:\", label)\n\n# If labeled, select best track and save its tracking video\nif label in [\"crossing\", \"not_crossing\"] and len(track_f) > 0:\n    best_tid, score = select_best_track(track_f)\n    best_path = f\"{OUT_ROOT}/{video_id}_tracking_best_tid{best_tid}.mp4\"\n    save_tracking_video(frames, track_f, best_path, fps=10, only_tids={best_tid}, title=f\"BEST TRACK {best_tid}\")\n    print(\"Best tid:\", best_tid, \"score:\", score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_original_vs_crop(frames, track_seq, expand=EXPAND_RATIO):\n    # take 1st, middle, last\n    picks = [track_seq[0], track_seq[len(track_seq)//2], track_seq[-1]]\n    plt.figure(figsize=(12,6))\n\n    for i, (fidx, box, conf) in enumerate(picks):\n        orig = cv2.imread(frames[fidx])\n        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n        x1,y1,x2,y2 = map(int, box)\n        orig2 = orig.copy()\n        cv2.rectangle(orig2, (x1,y1),(x2,y2),(0,255,0),2)\n\n        crop = crop_with_context(frames[fidx], box, expand)\n\n        plt.subplot(2,3,i+1)\n        plt.imshow(orig2)\n        plt.title(f\"Original f{fidx}\")\n        plt.axis(\"off\")\n\n        plt.subplot(2,3,3+i+1)\n        plt.imshow(crop)\n        plt.title(f\"Crop f{fidx}\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nif label in [\"crossing\",\"not_crossing\", \"unknown\"] and len(track_f)>0:\n    best_tid, _ = select_best_track(track_f)\n    seq = track_f[best_tid]\n    show_original_vs_crop(frames, seq, expand=EXPAND_RATIO)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cleaning XML ","metadata":{}},{"cell_type":"code","source":"import glob\nimport os\nimport pandas as pd\nimport xml.etree.ElementTree as ET\n\nXML_GLOB = \"/kaggle/input/attributes-label/annotations_attributes/video_*_*.xml\"\nOUT_CSV  = \"/kaggle/working/clean_video_labels.csv\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_xml_peds(xml_path):\n    rows = []\n    base = os.path.splitext(os.path.basename(xml_path))[0]\n\n    # Normalize ID: video_0001_attributes ‚Üí video_0001\n    video_id = base.replace(\"_attributes\", \"\")\n\n    try:\n        root = ET.parse(xml_path).getroot()\n    except Exception:\n        return rows\n\n    for ped in root.findall(\".//pedestrian\"):\n        try:\n            crossing = int(ped.attrib.get(\"crossing\", -1))\n        except:\n            crossing = -1\n\n        rows.append({\n            \"video_id\": video_id,\n            \"crossing\": crossing\n        })\n\n    return rows\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xml_files = sorted(glob.glob(XML_GLOB))\nprint(\"XML files found:\", len(xml_files))\n\nrows = []\nfor xp in xml_files:\n    rows.extend(parse_xml_peds(xp))\n\ndf_peds = pd.DataFrame(rows)\nprint(\"Total pedestrian annotations:\", len(df_peds))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_peds = df_peds[df_peds[\"crossing\"].isin([0, 1])].copy()\n\nprint(\"After removing unknown (-1):\", len(df_peds))\nprint(df_peds[\"crossing\"].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_video_labels = (\n    df_peds\n    .groupby(\"video_id\")[\"crossing\"]\n    .apply(lambda s: 1 if (s == 1).any() else 0)\n    .reset_index()\n)\n\ndf_video_labels.columns = [\"video_id\", \"label\"]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_video_labels.to_csv(OUT_CSV, index=False)\n\nprint(\"Saved CSV to:\", OUT_CSV)\nprint(\"Total labeled videos:\", len(df_video_labels))\nprint(df_video_labels[\"label\"].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking again ","metadata":{}},{"cell_type":"code","source":"import os, glob, random\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom ultralytics import YOLO\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LABELS_CSV = \"/kaggle/working/clean_video_labels.csv\"\nlabels_df = pd.read_csv(LABELS_CSV)\n\nlabel_map = dict(zip(labels_df[\"video_id\"], labels_df[\"label\"]))\nprint(\"Loaded labels:\", len(label_map))\nprint(labels_df[\"label\"].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_video_ids(split):\n    img_dir = f\"{IMG_ROOT}/{split}\"\n    return sorted(set(f.split(\"_f\")[0] for f in os.listdir(img_dir)))\n\ndef list_frames(split, video_id):\n    img_dir = f\"{IMG_ROOT}/{split}\"\n    return sorted(\n        os.path.join(img_dir, f)\n        for f in os.listdir(img_dir)\n        if f.startswith(video_id)\n    )\n\nsplit = \"train\"\nvideo_ids = [v for v in list_video_ids(split) if v in label_map]\nvideo_id = random.choice(video_ids)\n\nframes = list_frames(split, video_id)\nlabel = label_map[video_id]\n\nprint(video_id, \"frames:\", len(frames), \"label:\", label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = cv2.imread(frames[len(frames)//2])\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img); plt.axis(\"off\"); plt.title(f\"{video_id} | label={label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONF = 0.33\nMIN_TRACK_LEN = 8\nMIN_STABILITY_IOU = 0.25\nTRACKER = \"botsort.yaml\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_tracking(frames):\n    yolo_local = YOLO(MODEL_PATH)  # reset tracker\n    track_db = {}\n\n    for fidx, fp in enumerate(frames):\n        img = cv2.imread(fp)\n        if img is None:\n            continue\n\n        r = yolo_local.track(\n            img,\n            conf=CONF,\n            imgsz=IMGSZ,\n            persist=True,\n            tracker=TRACKER,\n            verbose=False\n        )[0]\n\n        if r.boxes is None or r.boxes.id is None:\n            continue\n\n        boxes = r.boxes.xyxy.cpu().numpy()\n        ids   = r.boxes.id.cpu().numpy().astype(int)\n        confs = r.boxes.conf.cpu().numpy()\n\n        seen = set()\n        for box, tid, c in zip(boxes, ids, confs):\n            if tid == -1 or tid in seen:\n                continue\n            seen.add(tid)\n            track_db.setdefault(tid, []).append((fidx, box, float(c)))\n\n    return track_db\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_tracks(track_db):\n    t = {k:v for k,v in track_db.items() if len(v) >= MIN_TRACK_LEN}\n    t = {k:v for k,v in t.items() if track_stability(v) >= MIN_STABILITY_IOU}\n    return suppress_duplicates(t)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"track_f = filter_tracks(track_raw)\n\nsave_tracking_video(\n    frames,\n    track_f,\n    f\"{OUT_ROOT}/{video_id}_FILTERED.mp4\",\n    title=\"FILTERED TRACKS\"\n)\n\nprint(\"Filtered tracks:\", len(track_f))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_tid = max(track_f, key=lambda t: track_score(track_f[t]))\nbest_seq = track_f[best_tid]\nprint(\"Best track ID:\", best_tid)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_seq = sorted(best_seq, key=lambda x: x[0])\nlocked_track = {best_tid: best_seq}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_tracking_video(\n    frames,\n    locked_track,\n    f\"{OUT_ROOT}/{video_id}_BEST_LOCKED.mp4\",\n    title=f\"BEST LOCKED | label={label}\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_with_context(frame_path, box, expand_ratio=1.8):\n    img = cv2.imread(frame_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    H, W = img.shape[:2]\n\n    x1,y1,x2,y2 = map(float, box)\n    cx,cy = (x1+x2)/2,(y1+y2)/2\n    bw,bh = (x2-x1)*expand_ratio,(y2-y1)*expand_ratio\n\n    nx1 = int(max(0, cx-bw/2))\n    ny1 = int(max(0, cy-bh/2))\n    nx2 = int(min(W, cx+bw/2))\n    ny2 = int(min(H, cy+bh/2))\n\n    return img[ny1:ny2, nx1:nx2]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samples = [best_seq[0], best_seq[len(best_seq)//2], best_seq[-1]]\n\nplt.figure(figsize=(12,4))\nfor i,(fidx,box,_) in enumerate(samples):\n    crop = crop_with_context(frames[fidx], box)\n    plt.subplot(1,3,i+1)\n    plt.imshow(crop); plt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_windows(seq, seq_len=16, stride=4):\n    if len(seq) < seq_len:\n        return []\n    return [seq[i:i+seq_len] for i in range(0, len(seq)-seq_len+1, stride)]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"windows = build_windows(best_seq, SEQ_LEN, STRIDE)\nprint(\"Windows:\", len(windows))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w = windows[len(windows)//2]\n\nplt.figure(figsize=(12,4))\nfor i,(fidx,box,_) in enumerate(w[:3]):\n    crop = crop_with_context(frames[fidx], box)\n    plt.subplot(1,3,i+1)\n    plt.imshow(crop); plt.axis(\"off\")\nplt.suptitle(f\"Sequence sample | label={label}\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}